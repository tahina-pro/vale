///////////////////////////////////////////////////////////////////////////////
//
// Based on sha256-armv4.pl from OpenSSL 1.1.1-dev
// See https://github.com/openssl/openssl/blob/master/crypto/sha/asm/sha256-armv4.pl
// The original file contains the following notices:
//
// # ==================================================================== 
// # Copyright 2007-2016 The OpenSSL Project Authors. All Rights Reserved. 
// # 
// # Licensed under the OpenSSL license (the "License").  You may not use 
// # this file except in compliance with the License.  You can obtain a copy 
// # in the file LICENSE in the source distribution or at 
// # https://www.openssl.org/source/license.html 
// # 
// # ==================================================================== 
// # Written by Andy Polyakov <appro@openssl.org> for the OpenSSL 
// # project. The module is, however, dual licensed under OpenSSL and 
// # CRYPTOGAMS licenses depending on where you obtain it. For further 
// # details see http://www.openssl.org/~appro/cryptogams/. 
// # 
// # Permission to use under GPL terms is granted. 
// # ==================================================================== 
//
///////////////////////////////////////////////////////////////////////////////

include{:verbatim}{:from BASE} "arch/arm/globals.s.dfy"
include{:verbatim}{:from BASE} "arch/arm/print.s.dfy"
include{:verbatim}{:from BASE} "arch/arm/vale.i.dfy"
include{:verbatim}{:from BASE} "lib/util/words_and_bytes.s.dfy"
include{:verbatim}{:from BASE} "lib/util/dafny_wrappers.i.dfy"

include{:verbatim}{:from BASE} "crypto/hashing/sha256.i.dfy"
include{:verbatim}{:from BASE} "crypto/hashing/sha256_helpers.i.dfy"
include{:verbatim}{:from BASE} "crypto/hashing/sha-arm/bit-vector-lemmas.i.dfy"
include{:verbatim}{:from BASE} "crypto/hashing/sha-arm/sha256-arm-helpers.i.dfy"
include{:verbatim}{:from BASE} "crypto/hashing/sha-arm/sha256-invariants.i.dfy"
include{:from BASE} "arch/arm/decls.vad"

#verbatim

module ARM_sha256 {

import opened ARM_decls_i
import opened ARM_vale_i
import opened ARM_print_s
import opened sha256_i
import opened sha256_helpers_i
import opened bit_vector_lemmas_i
import opened sha256_arm_helpers_i
import opened sha256_invariants_i
import opened dafny_wrappers_i

function method Sigma0(i:int) : uint32
    requires 0 <= i < 3;
{
    [2, 13, 22][i]
}

function method Sigma1(i:int) : uint32
    requires 0 <= i < 3;
{
    [6, 11, 25][i]
}

function method sigma0(i:int) : uint32
    requires 0 <= i < 3;
{
    [7, 18, 3][i]
}

function method sigma1(i:int) : uint32
    requires 0 <= i < 3;
{
    [17, 19, 10][i]
}

type SHA_step = i | 0 <= i < 64

function method GetReg(r:int) : ARMReg
    requires 0 <= r <= 12;
{
         if r ==  0 then R0
    else if r ==  1 then R1
    else if r ==  2 then R2
    else if r ==  3 then R3
    else if r ==  4 then R4
    else if r ==  5 then R5
    else if r ==  6 then R6
    else if r ==  7 then R7
    else if r ==  8 then R8
    else if r ==  9 then R9
    else if r == 10 then R10
    else if r == 11 then R11
    else R12 
}

#endverbatim

procedure {:refined} {:timeLimitMultiplier 4} Body_00_15(
    inline i:SHA_step,
    inline perm:perm_index,
    inline input_slot:uint32,
    inline i_plus_2:uint32,
    inline i_plus_15:uint32,
    inline input_taint:taint,
    ghost input_ptr:uint32,
    ghost trace_in:SHA256Trace,
    ghost input:seq(uint32),
    inout t0:opr32,
    inout t1:opr32,
    inout t2:opr32,
    inout t3:opr32,
    inout t4:opr32,
    //inout inp:opr32,
    inout a:opr32,
          b:opr32,
          c:opr32,
    inout d:opr32,
          e:opr32,
          f:opr32,
          g:opr32,
    inout h:opr32
    ) returns (
    ghost trace_out:SHA256Trace
    )
requires/ensures
    //WordAligned(sp);
    ValidAddr(mem, sp + input_slot);
reads 
    sp; globals;
modifies 
    mem; lr;
requires {:refined false}
    @t0 == OReg(R0);
    @t1 == OReg(R2);
    @t2 == OReg(GetReg(if Even(i) then 12 else 3));
    @t3 == OReg(GetReg(if Even(i) then  3 else 12));
    @t4 == OReg(R1);
    //@inp == OReg(R1);
    @a  == OReg(GetReg(4+ApplyPerm(0, perm)));
    @b  == OReg(GetReg(4+ApplyPerm(1, perm)));
    @c  == OReg(GetReg(4+ApplyPerm(2, perm)));
    @d  == OReg(GetReg(4+ApplyPerm(3, perm)));
    @e  == OReg(GetReg(4+ApplyPerm(4, perm)));
    @f  == OReg(GetReg(4+ApplyPerm(5, perm)));
    @g  == OReg(GetReg(4+ApplyPerm(6, perm)));
    @h  == OReg(GetReg(4+ApplyPerm(7, perm)));
requires/ensures
    // Stack is writeable
    ValidAddrs(mem, sp, 19);

    // Ghost input matches in-memory input
    SeqLength(input) == 16;
    i < 16 ==>
        (input_ptr + 16*4 < 0x1_0000_0000)
     && (input_ptr + 16*4 < sp || sp + 19*4 <= input_ptr)    // input_ptr doesn't alias the stack
     && ValidSrcAddrs(mem, input_ptr, 16, input_taint)
     && InputMatchesMemory(input, input_ptr, 16, mem);
requires
    // K table adjusted properly
    ValidGlobalsAddr(globals, K_SHA256s().sym, lr);
    AddressOfGlobal(K_SHA256s()) + 4*64 < 0x1_0000_0000;    // We won't wrap around while accessing K_SHA256s
    lr == AddressOfGlobal(K_SHA256s()) + 4*i;
    SeqLength(globals[K_SHA256s()]) == 256;
    forall j :: 0 <= j < 64 ==> globals[K_SHA256s()][j] == K_SHA256(j);

    i < 16 ==> t4 == input_ptr + (i+1)*4;

    ValidSrcAddrs(mem, sp, (if i < 16 then i else 16), input_taint);

    input_slot == CheapMod16(i)*4;

    i >= 15 ==> i_plus_2  == CheapMod16(i+ 2)*4 && ValidSrcAddr(mem, sp+i_plus_2, input_taint);
    i >= 15 ==> i_plus_15 == CheapMod16(i+15)*4 && ValidSrcAddr(mem, sp+i_plus_15, input_taint);

    // SHA semantics
    SeqLength(trace_in.H) > 0;
    IsSHA256TraceReadyForStep(trace_in, i);
    if i == 0 || i >= 16 then last(last(trace_in.atoh)) == atoh_c(a, b, c, d, e, f, g, h)
    else last(last(trace_in.atoh)) == atoh_c(BitwiseAdd32(a, t2), b, c, d, e, f, g, h);

    // t1 holds the current value of W
    t1 == (if (i < 16) then input[i] else last(trace_in.W)[i]);

    // The first 16 values in W are the byte-swapped version of the input uint32s
    forall j :: 0 <= j < 16 ==> last(trace_in.W)[j] == bswap32(input[j]);

    // All previous Ws are in memory where we expect them
    WsMatchMemory(trace_in, i, sp, mem);

    // SHA tactics
    t3 == BitwiseXor(b, c);
    i >= 16 ==> RotateRight(t0, Sigma1(0)) == BSIG1(e);

ensures
    t2 == BitwiseXor(a, b);
    a == (if 0 < i < 16 then old(BitwiseAdd32(a, t2)) else old(a));
    i >= 15 ==> t4 == mem[sp+i_plus_15].v;
    lr == old(lr) + 4;

    // Updated input ptr
    if i < 15 then t4 == input_ptr + (i+2)*4              // Advanced input ptr
    else 
        t4 == mem[sp + i_plus_15].v
     && (if i == 15 then mem[sp+17*4].v == input_ptr+16*4   // We stored the advanced input ptr on the stack
                      && mem[sp+17*4].t == Public
         else mem[sp+17*4] == old(mem[sp+17*4]));               // We preserved the input ptr on the stack

    // Memory framing: We only touch the stack
    forall addr:uint32 :: old(mem)?[addr] && (addr < sp || addr >= sp + 19*4) ==> mem?[addr] && mem[addr] == old(mem)[addr];
    mem[sp + 16*4] == old(mem)[sp + 16*4];
    mem[sp + 18*4] == old(mem)[sp + 18*4];

    ValidSrcAddrs(mem, sp, (if i + 1 < 16 then i + 1 else 16), input_taint);

    IsSHA256TraceReadyForStep(trace_out, i+1);
    trace_out.M == trace_in.M;
    trace_out.H == trace_in.H;
    trace_out.W == trace_in.W;
    // t1 holds the next value of W
    t1 == (if i + 1 < 16 then input[i + 1] else if i + 1 <= 64 then mem[sp + i_plus_2].v else t1);
    WsMatchMemory(trace_out, i+1, sp, mem);
     // The atohs almost match the outgoing variables
    last(last(trace_out.atoh)) == atoh_c(BitwiseAdd32(h, t3), a, b, c, d, e, f, g);
{
    //assert input_slot < 68;
    inline if (i < 16) {
        inline if (i == 15) {
            // Save a copy of the incremented input pointer, so we can free up t4
            assert 68 == 17*4;
            rSTR(t4, sp, 68, Public); 
        }

        rEORShift(t0, e, e, RORShift(Sigma1(1) - Sigma1(0)));
        // Optimize the first case.  This is an optimization that OpenSSL misses! 
        inline if (i != 0) {
            rADDWrap(a, a, t2);  // h += Maj(a,b,c) from the past?
        }
        rEORShift(t0, t0, e, RORShift(Sigma1(2) - Sigma1(0)));   // Sigma1(e)
        rREV(t1, t1);
    }

    rLDRglobal(t2, K_SHA256s().sym, lr, 0);
    rADDWrap(lr, lr, 4);    // TODO: OpenSSL does this in one instruction with a load-and-increment.
    lemma_mod_in_bounds2(i, AddressOfGlobal(K_SHA256s()), old(lr), lr);
    rADDWrap(h, h, t1);      //  h+=X[i]  BP: X[i] = input[i]?
    rSTR(t1, sp, input_slot, input_taint);  // @ BP: Save a copy of W[i] for use in subsequent W calculations
    rEOR(t1, f, g);
    ghost var old_h := h;
    rADDWrapShift(h, h, t0, RORShift(Sigma1(0))); // h += Sigma1(e)

    // Prove that we computed Sigma1(e) correctly:
    forall :: h == BitwiseAdd32(old_h, BSIG1(e))
    {
        reveal BSIG1;
        lemma_RotateRightCommutesXor(e, 6, 11, 25);
    }

    rAND(t1, t1, e);
    rADDWrap(h, h, t2);  // h += K256(i)
    rEOR(t1, t1, g);     // Ch(e,f,g)

    assert t1 == Ch(e, f, g) by { lemma_Ch(e, f, g, t1); }

    rEORShift(t0, a, a, RORShift(Sigma0(1) - Sigma0(0)));
    rADDWrap(h, h, t1);  // h += Ch(e,f,g)

    ghost var old_t1 := if i < 16 then bswap32(old(t1)) else old(t1);
    assert h == BitwiseAdd32(BitwiseAdd32(BitwiseAdd32(BitwiseAdd32(old(h), old_t1), BSIG1(e)), K_SHA256(i)), Ch(e, f, g));
    lemma_BitwiseAdd32Associates5(old(h), old_t1, BSIG1(e), K_SHA256(i), Ch(e, f, g), h);

//    #if $i==31
//        and  $t2,$t2,#0xff
//        cmp  $t2,#0xf2      @ done?
//    #endif

    inline if (i < 15) {
         assert ValidAddr(old(mem), t4); // OBSERVE
         rLDR(t1, t4, 0, input_taint);    // Prefetch
         rADDWrap(t4, t4, 4);   // Advance to the next input  // TODO: OpenSSL does this in one instruction with a load-and-increment
         lemma_mod_in_bounds(i, input_ptr, old(t4), t4);
//         assert old(t4) + 4 == input_ptr + (i+1)*4 + 4 == input_ptr + (i+2)*4;
//         assert input_ptr + (i+2)*4 < 0x1_0000_0000;
         assert t4 == old(t4) + 4;
         rEOR(t2, a, b);            //  a^b, b^c in next round
    } else {
         assert ValidAddr(old(mem), sp+i_plus_2);  // OBSERVE
         assert ValidAddr(old(mem), sp+i_plus_15); // OBSERVE
         rLDR(t1, sp, i_plus_2, input_taint);     // @ from future BODY_16_xx 
         rEOR(t2, a, b);             //  a^b, b^c in next round
         rLDR(t4, sp, i_plus_15, input_taint);    // @ from future BODY_16_xx
    }
    rEORShift(t0,t0,a, RORShift(Sigma0(2)-Sigma0(0))); // Sigma0(a)
    rAND(t3,t3,t2);      // (b^c)&=(a^b)
    rADDWrap(d,d,h);     // d+=h
    rEOR(t3,t3,b);       // Maj(a,b,c)
    assert t3 == Maj(a,b,c) by { lemma_Maj(a, b, c, t3); }
    old_h := h;
    rADDWrapShift(h,h,t0, RORShift(Sigma0(0)));   // h+=Sigma0(a)

    // Prove we computed Sigma0(a) correctly:
    forall :: h == BitwiseAdd32(old_h, BSIG0(a))
    {
        reveal BSIG0;
        lemma_RotateRightCommutesXor(a, 2, 13, 22);
    }

    ghost var T1 := BitwiseAdd32(BitwiseAdd32(BitwiseAdd32(BitwiseAdd32(old(h), BSIG1(e)),
                                                           Ch(e,f,g)),
                                              K_SHA256(i)),
                                 old_t1);
    //assert h == BitwiseAdd32(T1, BSIG0(a));
    assert BitwiseAdd32(h, t3) == BitwiseAdd32(T1, BitwiseAdd32(BSIG0(a), t3)) by
           { lemma_BitwiseAdd32Associates3'(T1, BSIG0(a), t3); }

    // Construct a trace_out
    ghost var old_a := if i == 0 || i >= 16 then old(a) else old(BitwiseAdd32(a, t2));
    ghost var old_atoh := old(atoh_c(old_a, b, c, d, e, f, g, h));
    ghost var new_atoh := atoh_c(BitwiseAdd32(h, t3), old_a, old(b), old(c), d, old(e), old(f), old(g));
    lemma_BitwiseAdd32_properties(old(a));
    assert i == 0 ==> BitwiseAdd32(old(a), 0) == old(a);   // OBSERVE
    assert old_atoh.a == a;     // OBSERVE
 
    ghost var new_atoh_list := last(trace_in.atoh) + seq(new_atoh);
    trace_out := trace_in.(atoh := SeqDrop(trace_in.atoh, SeqLength(trace_in.H)-1) + seq(trace_in.atoh[SeqLength(trace_in.H)-1] + seq(new_atoh)));

    // OBSERVE: Triggers
    assert TBlk(SeqLength(trace_in.H)-1) && TBlk(SeqLength(trace_in.H)) && TStep(i) && TStep(i + 1);
    ghost var superfluous_state_in  := SHA256_state_c(last(trace_in.H), last(trace_in.W), old_atoh);
    ghost var superfluous_state_out := SHA256_state_c(last(trace_out.H), last(trace_out.W), new_atoh);
    lemma_SHA256TransitionOKAfterSettingAtoH(trace_in, superfluous_state_in, trace_out, superfluous_state_out, i);

    assert IsSHA256TraceReadyForStep(trace_out, i+1);

    // Prove that stack is still valid
    lemma_ValidAddrsPreservation(old(mem), mem, sp, 19, sp + 68, sp + input_slot);

    // Help prove that the input is still intact
    ghost if (i < 16) {
        lemma_InputPreservation(old(mem), mem, input, input_ptr, 16, sp + 68, sp + input_slot);

        // Prove input is still valid src
        lemma_ValidSrcAddrsPreservation(old(mem), mem, input_ptr, 16, input_taint, sp + 68, sp + input_slot);
     }
 
    // Prove sp is becoming valid src
    lemma_ValidSrcAddrsIncrement(old(mem), mem, sp, 16, input_taint, i, sp + 68, sp + input_slot);

    // Prove we updated the Ws correctly
    lemma_WsIncrement(old(mem), mem, trace_in, trace_out, sp, i, sp + 68, sp + input_slot);

//    assert { :split_here} true;
    assert ValidAddr(old(mem), sp + 16*4); // OBSERVE
    assert ValidAddr(old(mem), sp + 18*4); // OBSERVE
    assert mem[sp + 18*4] == old(mem)[sp + 18*4];
}

#verbatim

predicate Body_00_15LoopStateInvariantBreakdown(
    orig_mem:memmap,
    mem:memmap,
    input_ptr:uint32,
    input_taint:taint,
    orig_trace:SHA256Trace,
    current_trace:SHA256Trace,
    i:int,
    sp:uint32,
    globals:map<operand,seq<uint32>>,
    lr:uint32,
    t1:uint32,
    t2:uint32,
    t3:uint32,
    t4:uint32,
    input:seq<uint32>,
    a:uint32, b:uint32, c:uint32, d:uint32, e:uint32, f:uint32, g:uint32, h:uint32
    )
{
    0 <= i <= 16
 // Stack is accessible in both old and new mem
 && ValidAddrs(orig_mem, sp, 19)
 && ValidAddrs(mem, sp, 19)
 && ValidSrcAddrs(mem, sp, if i < 16 then i else 16, input_taint)
 && ValidAddr(mem, sp + CheapMod16(i)*4)
 && ValidAddr(mem, sp + CheapMod16(i+9)*4)

    // K table adjusted properly
 && ValidGlobalsAddr(globals, K_SHA256s().sym, lr)
 && K_SHA256s() in globals
 && AddressOfGlobal(K_SHA256s()) + 4*64 < 0x1_0000_0000    // We won't wrap around while accessing K_SHA256s
 && lr == AddressOfGlobal(K_SHA256s()) + 4*i
 && |globals[K_SHA256s()]| == 256
 && (forall j :: 0 <= j < 64 ==> globals[K_SHA256s()][j] == K_SHA256(j))

    // Ghost input matches in-memory input
 && SeqLength(input) == 16
 && (i < 16 ==>
        (input_ptr + 16*4 < 0x1_0000_0000)
     && (input_ptr + 16*4 < sp || sp + 19*4 <= input_ptr)    // input_ptr doesn't alias the stack
     && ValidSrcAddrs(mem, input_ptr, 16, input_taint)
     && InputMatchesMemory(input, input_ptr, 16, mem)
    )

 && (i >= 15 ==> ValidSrcAddr(mem, sp + CheapMod16(i +  2)*4, input_taint)
              && ValidSrcAddr(mem, sp + CheapMod16(i + 15)*4, input_taint))

 && t3 == BitwiseXor(b, c)

    // Memory framing: We only touch the stack
 && (forall addr:uint32 :: addr in orig_mem && (addr < sp || addr >= sp + 19*4) ==> addr in mem && mem[addr] == orig_mem[addr])
 && mem[sp + 16*4] == orig_mem[sp + 16*4]
 && mem[sp + 18*4] == orig_mem[sp + 18*4]

    // SHA semantics
 && SeqLength(current_trace.H) > 0
 && IsSHA256TraceReadyForStep(current_trace, i)
 && current_trace.M == orig_trace.M
 && current_trace.H == orig_trace.H
 && current_trace.W == orig_trace.W
 && (last(last(current_trace.atoh)) == 
        if i == 0 then 
            atoh_c(a, b, c, d, e, f, g, h)
        else 
            atoh_c(BitwiseAdd32(a, t2), b, c, d, e, f, g, h))

    // t1 holds the current value of W
 && t1 == (if (i < 16) then input[i] else if i + 1 <= 64 then mem[sp + CheapMod16(i+1)*4].v else last(current_trace.W)[i])

    // The first 16 values in W are the byte-swapped version of the input uint32s
 && (forall j :: 0 <= j < 16 ==> last(current_trace.W)[j] == bswap32(input[j]))

    // All previous Ws are in memory where we expect them
 && (16 <= i < 64 ==> (forall j :: i - 16 <= j < i ==> last(current_trace.W)[j] == mem[sp + CheapMod16(j)*4].v))
 && (i < 16 ==> (forall j :: 0 <= j < i ==> last(current_trace.W)[j] == mem[sp + j*4].v))
 && (i < 15  ==> ValidAddr(mem, t4) && mem[t4].v == input[i+1])
 && (i >= 15 ==> ValidAddr(mem, sp+CheapMod16(i+ 2)*4))
 && (i >= 15 ==> ValidAddr(mem, sp+CheapMod16(i+15)*4))

     // Updated input ptr
 && (if i < 16 then t4 == input_ptr + (i+1)*4         // Correctly advanced input ptr
     else mem[sp+17*4].v == input_ptr + 4*16          // We preserved the advanced input ptr on the stack 
       && mem[sp+17*4].t == Public
    ) 

 && (i >= 16 ==> t4 == mem[sp + CheapMod16(i+14)*4].v)
}

#endverbatim

procedure {:refined} {:recursive} {:timeLimitMultiplier 2} Body_00_15UnrolledRecursive(
    inline n:int,
    inline i:int,
    inline perm:perm_index,
    inline input_taint:taint,
    ghost input_ptr:uint32,
    ghost orig_trace:SHA256Trace,
    ghost trace_in:SHA256Trace,
    ghost input:seq(uint32),
    inout t0:opr32,
    inout t1:opr32,
    inout t2:opr32,
    inout t3:opr32,
    inout t4:opr32,
    inout a:opr32,
    inout b:opr32,
    inout c:opr32,
    inout d:opr32,
    inout e:opr32,
    inout f:opr32,
    inout g:opr32,
    inout h:opr32
    ) returns (
    ghost trace_out:SHA256Trace
    )
  requires
    0 <= n <= 16;
    n == 16 - i;
    perm == OpaqueMod(i, 8);
  requires {:refined false}
    @t0 == OReg(R0);
    @t1 == OReg(R2);
    @t2 == OReg(GetReg(if Even(i) then 12 else 3));
    @t3 == OReg(GetReg(if Even(i) then  3 else 12));
    @t4 == OReg(R1);
    //@inp == OReg(R1);
    @a  == OReg(GetReg(4+ApplyPerm(0, perm)));
    @b  == OReg(GetReg(4+ApplyPerm(1, perm)));
    @c  == OReg(GetReg(4+ApplyPerm(2, perm)));
    @d  == OReg(GetReg(4+ApplyPerm(3, perm)));
    @e  == OReg(GetReg(4+ApplyPerm(4, perm)));
    @f  == OReg(GetReg(4+ApplyPerm(5, perm)));
    @g  == OReg(GetReg(4+ApplyPerm(6, perm)));
    @h  == OReg(GetReg(4+ApplyPerm(7, perm)));
  requires
    Body_00_15LoopStateInvariantBreakdown(mem, mem, input_ptr, input_taint, orig_trace, trace_in, i, sp, globals, lr, 
                                          t1, t2, t3, t4, input,
                                          a, b, c, d, e, f, g, h);
  reads
    sp; globals;
  modifies
    mem; lr;
  ensures
    let arr := seq8(a, b, c, d, e, f, g, h) in
        Body_00_15LoopStateInvariantBreakdown(old(mem), mem, input_ptr, input_taint, orig_trace, trace_out, 16, sp, globals, lr,
                                              t1, if Even(n) then t2 else t3, if Even(n) then t3 else t2, t4, input,
                                              SelectPerm(arr, 0, perm), SelectPerm(arr, 1, perm), SelectPerm(arr, 2, perm),
                                              SelectPerm(arr, 3, perm), SelectPerm(arr, 4, perm), SelectPerm(arr, 5, perm),
                                              SelectPerm(arr, 6, perm), SelectPerm(arr, 7, perm));
{
    inline if (0 < n <= 16 && 0 <= i < 16) {
        assert OpaqueMod(i + 1, 8) == (if perm == 7 then 0 else perm + 1) by { reveal OpaqueMod; }
        ghost var trace_mid:SHA256Trace;
        trace_mid := Body_00_15(i, perm,
                                CheapMod16(i)*4, CheapMod16(i+2)*4, CheapMod16(i+15)*4,
                                input_taint, input_ptr, trace_in, input,
                                t0, t1, t2, t3, t4,
                                a, b, c, d, e, f, g, h);
        trace_out := Body_00_15UnrolledRecursive(n-1, i+1, if perm == 7 then 0 else perm + 1,
                                                 input_taint, input_ptr, orig_trace, trace_mid, input,
                                                 t0, t1, t3, t2, t4,
                                                 h, a, b, c, d, e, f, g);
    }
    else {
        assert OpaqueMod(i, 8) == 0 by { reveal OpaqueMod; }
        trace_out := trace_in;
    }
}

procedure {:refined} Body_00_15LoopUnrolled(
    ghost input_ptr:uint32,
    ghost trace_in:SHA256Trace,
    ghost input:seq(uint32),
    inline input_taint:taint
    ) returns (
    ghost trace_out:SHA256Trace
    )
    requires Body_00_15LoopStateInvariantBreakdown(mem, mem, input_ptr, input_taint, trace_in, trace_in, 0, sp, globals, lr, 
                                                   r2, r12, r3, r1, input,
                                                   r4, r5, r6, r7, r8, r9, r10, r11);
    reads  sp; globals;
    modifies mem; lr; r0; r1; r2; r3; r4; r5; r6; r7; r8; r9; r10; r11; r12;
    ensures  Body_00_15LoopStateInvariantBreakdown(old(mem), mem, input_ptr, input_taint, trace_in, trace_out, 16, sp, globals, lr,
                                                   r2, r12, r3, r1, input,
                                                   r4, r5, r6, r7, r8, r9, r10, r11);
{
    assert OpaqueMod(0, 8) == 0 by { reveal OpaqueMod; }
    trace_out := Body_00_15UnrolledRecursive(16, 0, 0, input_taint, input_ptr, trace_in, trace_in, input,
                                             r0, r2, r12, r3, r1,
                                             r4, r5, r6, r7, r8, r9, r10, r11);
}

procedure {:refined} {:timeLimitMultiplier 3} Body_16_XX( 
    inline i:SHA_step,
    inline perm:perm_index,
    inline input_slot:uint32,
    inline input_slot_9:uint32,
    inline input_taint:taint,
    ghost trace_in:SHA256Trace,
    ghost input:seq(uint32),
    inout t0:opr32,
    inout t1:opr32,
    inout t2:opr32,
    inout t3:opr32,
    inout t4:opr32,
    inout a:opr32,
          b:opr32,
          c:opr32,
    inout d:opr32,
          e:opr32,
          f:opr32,
          g:opr32,
    inout h:opr32)

requires/ensures
    // Stack is accessible
    ValidAddrs(mem, sp, 19);
    ValidSrcAddrs(mem, sp, 16, input_taint);
reads 
    sp; globals;
modifies 
    mem; lr;
requires {:refined false}
    @t0 == OReg(R0);
    @t1 == OReg(R2);
    @t2 == OReg(GetReg(if Even(i) then 12 else 3));
    @t3 == OReg(GetReg(if Even(i) then  3 else 12));
    @t4 == OReg(R1);
    //@inp == OReg(R1);
    @a  == OReg(GetReg(4+ApplyPerm(0, perm)));
    @b  == OReg(GetReg(4+ApplyPerm(1, perm)));
    @c  == OReg(GetReg(4+ApplyPerm(2, perm)));
    @d  == OReg(GetReg(4+ApplyPerm(3, perm)));
    @e  == OReg(GetReg(4+ApplyPerm(4, perm)));
    @f  == OReg(GetReg(4+ApplyPerm(5, perm)));
    @g  == OReg(GetReg(4+ApplyPerm(6, perm)));
    @h  == OReg(GetReg(4+ApplyPerm(7, perm)));

requires
    i >= 16;
    input_slot == CheapMod16(i)*4;
    input_slot_9 == CheapMod16(i+9)*4;
    ValidAddr(mem, sp + input_slot);
    ValidAddr(mem, sp + input_slot_9); 

    // K table adjusted properly
    ValidGlobalsAddr(globals, K_SHA256s().sym, lr);
    AddressOfGlobal(K_SHA256s()) + 4*64 < 0x1_0000_0000;    // We won't wrap around while accessing K_SHA256s
    lr == AddressOfGlobal(K_SHA256s()) + 4*i;
    SeqLength(globals[K_SHA256s()]) == 256;
    forall j :: 0 <= j < 64 ==> globals[K_SHA256s()][j] == K_SHA256(j);

    SeqLength(input) == 16;

    ValidAddr(mem, sp + CheapMod16(i +  2)*4);
    ValidAddr(mem, sp + CheapMod16(i + 15)*4);

    t3 == BitwiseXor(b, c);

    // SHA semantics
    SeqLength(trace_in.H) > 0;
    IsSHA256TraceReadyForStep(trace_in, i);
    last(last(trace_in.atoh)) == atoh_c(BitwiseAdd32(a, t2), b, c, d, e, f, g, h);

    // The first 16 values in W are the byte-swapped version of the input uint32s
    forall j :: 0 <= j < 16 ==> last(trace_in.W)[j] == bswap32(input[j]);

    // All previous Ws are in memory where we expect them
    16 <= i < 64 ==> (forall j :: i - 16 <= j < i ==> last(trace_in.W)[j] == mem[sp + CheapMod16(j)*4].v);

    // t1 and t4 should already hold previous W values
    t1 == mem[sp + CheapMod16(i+1)*4].v;
    t4 == mem[sp + CheapMod16(i+14)*4].v;
ensures 
    t2 == BitwiseXor(a, b);
    lr == old(lr) + 4;
    t4 == mem[sp+CheapMod16(i + 15)*4].v;

    mem[sp+17*4] == old(mem[sp+17*4]);         // We preserved the input ptr on the stack

    // Memory framing: We only touch the stack
    forall addr:uint32 :: old(mem)?[addr] && (addr < sp || addr >= sp + 19*4) ==> mem?[addr] && mem[addr] == old(mem)[addr];
    mem[sp + 16*4] == old(mem)[sp + 16*4];
    mem[sp + 18*4] == old(mem)[sp + 18*4];

    exists trace_out:SHA256Trace ::
        IsSHA256TraceReadyForStep(trace_out, i+1)
     && trace_out.M == trace_in.M
     && trace_out.H == trace_in.H
     && trace_out.W == trace_in.W
     // t1 holds the next value of W
     && t1 == (if i + 1 <= 64 then mem[sp + CheapMod16(i + 2)*4].v else t1) 

     // Remaining Ws are laid out in memory
     && (i + 1 < 64 ==> (forall j :: i+1 - 16 <= j < i+1 ==> last(trace_out.W)[j] == mem[sp + CheapMod16(j)*4].v))

     // The atohs almost match the outgoing variables
     && last(last(trace_out.atoh)) == atoh_c(BitwiseAdd32(h, t3), a, b, c, d, e, f, g);
{
    rMOVShift(t0, t1, RORShift(sigma0(0)));
    rADDWrap(a, a, t2);  // h+=Maj(a,b,c) from the past
    rMOVShift(t2, t4, RORShift(sigma1(0)));
    rEORShift(t0,t0,t1,RORShift(sigma0(1)));
    rEORShift(t2,t2,t4,RORShift(sigma1(1)));
    rEORShift(t0,t0,t1,LSRShift(sigma0(2)));     // sigma0(X[i+1])
    assert t0 == SSIG0(t1) by { reveal SSIG0; }

    rLDR(t1, sp, input_slot, input_taint);
    rEORShift(t2,t2,t4, LSRShift(sigma1(2)));     // sigma1(X[i+14])
    assert t2 == SSIG1(t4) by { reveal SSIG1; }
    rLDR(t4, sp, input_slot_9, input_taint);

    rADDWrap(t2,t2,t0);
    rEORShift(t0,e,e,RORShift(Sigma1(1)-Sigma1(0)));    // from BODY_00_15
    rADDWrap(t1,t1,t2);
    rEORShift(t0,t0,e,RORShift(Sigma1(2)-Sigma1(0)));  // Sigma1(e)  BP: Almost

    // Prove that we computed Sigma1(e) correctly:
    forall :: RotateRight(t0, Sigma1(0)) == BSIG1(e)
    {
        reveal BSIG1;
        lemma_RotateRightCommutesXor(e, 6, 11, 25);
    }

    rADDWrap(t1,t1,t4);      // X[i]

    // From the spec (and PartialSHA256TraceHasCorrectWs): 
    ghost var W := last(trace_in.W);
    assert TStep(i);
    assert W[i] == BitwiseAdd32(BitwiseAdd32(BitwiseAdd32(SSIG1(W[i-2]), W[i-7]), SSIG0(W[i-15])), W[i-16]);

    assert t1 == W[i] by {
        lemma_BitwiseAdd32Associates4(SSIG1(W[i-2]), W[i-7], SSIG0(W[i-15]), W[i-16], t1);
    }

    ghost var mid_a := a;
    ghost var mid_t2 := t2;
    ghost var dummy_input_ptr:uint32;  // Body_00_15 only cares about the value for i < 16
    ghost var trace_out:SHA256Trace;
    trace_out := Body_00_15(i, perm, input_slot, CheapMod16(i + 2)*4, CheapMod16(i + 15)*4, input_taint,
               dummy_input_ptr, trace_in, input,
               t0, t1, t2, t3, t4, 
               a, b, c, d, e, f, g, h);
    assert a == mid_a;
}

#verbatim

predicate{:opaque} Body_16_XXLoopStateInvariantBreakdown(
    orig_mem:memmap,
    mem:memmap,
    orig_trace:SHA256Trace,
    current_trace:SHA256Trace,
    i:int,
    sp:uint32,
    globals:map<operand,seq<uint32>>,
    lr:uint32,
    t1:uint32,
    t2:uint32,
    t3:uint32,
    t4:uint32,
    input_taint:taint,
    input:seq<uint32>,
    a:uint32, b:uint32, c:uint32, d:uint32, e:uint32, f:uint32, g:uint32, h:uint32
    )
{
    16 <= i <= 64
 && ValidAddrs(orig_mem, sp, 19)
 && ValidAddrs(mem, sp, 19)
 && ValidSrcAddrs(mem, sp, 16, input_taint)
 && ValidAddr(mem, sp + CheapMod16(i)*4)
 && ValidAddr(mem, sp + CheapMod16(i+9)*4)

    // K table adjusted properly
 && (i < 64 ==> ValidGlobalsAddr(globals, K_SHA256s().sym, lr))
 && K_SHA256s() in globals
 && AddressOfGlobal(K_SHA256s()) + 4*64 < 0x1_0000_0000    // We won't wrap around while accessing K_SHA256s
 && lr == AddressOfGlobal(K_SHA256s()) + 4*i
 && |globals[K_SHA256s()]| == 256
 && (forall j :: 0 <= j < 64 ==> globals[K_SHA256s()][j] == K_SHA256(j))

 && SeqLength(input) == 16

 && ValidAddr(mem, sp + CheapMod16(i +  2)*4)
 && ValidAddr(mem, sp + CheapMod16(i + 15)*4)

 && t3 == BitwiseXor(b, c)

    // Memory framing: We only touch the stack
 && (forall addr:uint32 :: addr in orig_mem && (addr < sp || addr >= sp + 19*4) ==> addr in mem && mem[addr] == orig_mem[addr])
 && mem[sp + 16*4] == orig_mem[sp + 16*4]
 && mem[sp + 17*4] == orig_mem[sp + 17*4]         // We preserved the input ptr on the stack
 && mem[sp + 18*4] == orig_mem[sp + 18*4]

    // SHA semantics
 && SeqLength(current_trace.H) > 0
 && IsSHA256TraceReadyForStep(current_trace, i)
 && current_trace.M == orig_trace.M
 && current_trace.H == orig_trace.H
 && current_trace.W == orig_trace.W
 && last(last(current_trace.atoh)) == atoh_c(BitwiseAdd32(a, t2), b, c, d, e, f, g, h)

    // The first 16 values in W are the byte-swapped version of the input uint32s
 && (forall j :: 0 <= j < 16 ==> last(current_trace.W)[j] == bswap32(input[j]))

    // All previous Ws are in memory where we expect them
 && (16 <= i < 64 ==> (forall j :: i - 16 <= j < i ==> last(current_trace.W)[j] == mem[sp + CheapMod16(j)*4].v))

    // t1 and t4 should already hold previous W values
 && (i < 64 ==> t1 == mem[sp + CheapMod16(i+1)*4].v)
 && t4 == mem[sp + CheapMod16(i+14)*4].v
}

#endverbatim

procedure {:refined} Body_16_XXWrap(
    inline i:SHA_step,
    inline perm:perm_index,
    inline input_slot:uint32,
    inline input_slot_9:uint32,
    inline input_taint:taint,
    ghost orig_mem:memmap,
    ghost orig_trace:SHA256Trace,
    ghost trace_in:SHA256Trace,
    ghost input:seq(uint32),
    inout t0:opr32,
    inout t1:opr32,
    inout t2:opr32,
    inout t3:opr32,
    inout t4:opr32,
    //inout inp:opr32,
    inout a:opr32,
          b:opr32,
          c:opr32,
    inout d:opr32,
          e:opr32,
          f:opr32,
          g:opr32,
    inout h:opr32)
    returns (ghost trace_out:SHA256Trace)
    reads
        sp; globals;
    modifies
        mem; lr;
    requires {:refined false}
        @t0 == OReg(R0);
        @t1 == OReg(R2);
        @t2 == OReg(GetReg(if Even(i) then 12 else 3));
        @t3 == OReg(GetReg(if Even(i) then  3 else 12));
        @t4 == OReg(R1);
        //@inp == OReg(R1);
        @a  == OReg(GetReg(4+ApplyPerm(0, perm)));
        @b  == OReg(GetReg(4+ApplyPerm(1, perm)));
        @c  == OReg(GetReg(4+ApplyPerm(2, perm)));
        @d  == OReg(GetReg(4+ApplyPerm(3, perm)));
        @e  == OReg(GetReg(4+ApplyPerm(4, perm)));
        @f  == OReg(GetReg(4+ApplyPerm(5, perm)));
        @g  == OReg(GetReg(4+ApplyPerm(6, perm)));
        @h  == OReg(GetReg(4+ApplyPerm(7, perm)));
    requires
        input_slot == CheapMod16(i)*4;
        input_slot_9 == CheapMod16(i+9)*4;
    requires Body_16_XXLoopStateInvariantBreakdown(orig_mem, mem, orig_trace, trace_in, i, sp, globals, lr, 
                                                   t1, t2, t3, t4, input_taint, input,
                                                   a, b, c, d, e, f, g, h);
    ensures  Body_16_XXLoopStateInvariantBreakdown(orig_mem, mem, orig_trace, trace_out, i + 1, sp, globals, lr,
                                                   t1, t3, t2, t4, input_taint, input,
                                                   h, a, b, c, d, e, f, g);
{
    reveal Body_16_XXLoopStateInvariantBreakdown;

    Body_16_XX(i, perm, input_slot, input_slot_9, input_taint, trace_in, input,
        t0, t1, t2, t3, t4, a, b, c, d, e, f, g, h);

    exists trace_out_tmp:SHA256Trace ::
            IsSHA256TraceReadyForStep(trace_out_tmp, i+1)
         && trace_out_tmp.M == trace_in.M
         && trace_out_tmp.H == trace_in.H
         && trace_out_tmp.W == trace_in.W
         && t1 == (if i + 1 <= 64 then mem[sp + CheapMod16(i + 2)*4].v else t1) 
         && (i + 1 < 64 ==> (forall j :: i+1 - 16 <= j < i+1 ==> last(trace_out_tmp.W)[j] == mem[sp + CheapMod16(j)*4].v))
         && last(last(trace_out_tmp.atoh)) == atoh_c(BitwiseAdd32(h, t3), a, b, c, d, e, f, g);

    trace_out := trace_out_tmp;
    assert Body_16_XXLoopStateInvariantBreakdown(old(mem), mem, trace_in, trace_out, i + 1, sp, globals, lr,
                                                       t1, t3, t2, t4, input_taint, input,
                                                       h, a, b, c, d, e, f, g);
}

procedure {:refined}{:recursive}{:timeLimitMultiplier 2} Body_16_XXUnroller(
    inline n:nat,
    inline i:int,
    inline perm:perm_index,
    inline input_slot:uint32,
    inline input_slot_9:uint32,
    inline input_taint:taint,
    ghost orig_mem:memmap,
    ghost orig_trace:SHA256Trace,
    ghost trace_in:SHA256Trace,
    ghost input:seq(uint32),
    inout t0:opr32,
    inout t1:opr32,
    inout t2:opr32,
    inout t3:opr32,
    inout t4:opr32,
    inout a:opr32,
    inout b:opr32,
    inout c:opr32,
    inout d:opr32,
    inout e:opr32,
    inout f:opr32,
    inout g:opr32,
    inout h:opr32)
    returns (ghost trace_out:SHA256Trace)
    reads
        sp; globals;
    modifies
        mem; lr;
    requires {:refined false}
        @t0 == OReg(R0);
        @t1 == OReg(R2);
        @t2 == OReg(GetReg(if Even(i) then 12 else 3));
        @t3 == OReg(GetReg(if Even(i) then  3 else 12));
        @t4 == OReg(R1);
        @a  == OReg(GetReg(4+ApplyPerm(0, perm)));
        @b  == OReg(GetReg(4+ApplyPerm(1, perm)));
        @c  == OReg(GetReg(4+ApplyPerm(2, perm)));
        @d  == OReg(GetReg(4+ApplyPerm(3, perm)));
        @e  == OReg(GetReg(4+ApplyPerm(4, perm)));
        @f  == OReg(GetReg(4+ApplyPerm(5, perm)));
        @g  == OReg(GetReg(4+ApplyPerm(6, perm)));
        @h  == OReg(GetReg(4+ApplyPerm(7, perm)));
        0 <= i <= 64;
        n == 64 - i;
    requires
        0 <= i <= 64;
        n == 64 - i;
        perm == OpaqueMod(i, 8);
        input_slot == CheapMod16(i)*4;
        input_slot_9 == CheapMod16(i+9)*4;
        Body_16_XXLoopStateInvariantBreakdown(orig_mem, mem, orig_trace, trace_in, i, sp, globals, lr, 
            t1, t2, t3, t4, input_taint, input, a, b, c, d, e, f, g, h);
    ensures
        let arr := seq8(a, b, c, d, e, f, g, h) in
            Body_16_XXLoopStateInvariantBreakdown(orig_mem, mem, orig_trace, trace_out, 64, sp, globals, lr,
                t1, if Even(i) then t2 else t3, if Even(i) then t3 else t2, t4, input_taint, input,
                SelectPerm(arr, 0, perm), SelectPerm(arr, 1, perm), SelectPerm(arr, 2, perm), SelectPerm(arr, 3, perm),
                SelectPerm(arr, 4, perm), SelectPerm(arr, 5, perm), SelectPerm(arr, 6, perm), SelectPerm(arr, 7, perm));
{
    inline if (n > 0 && 0 <= i < 64) {
        assert OpaqueMod(i + 1, 8) == (if perm == 7 then 0 else perm + 1) by { reveal OpaqueMod; }
        trace_out := Body_16_XXWrap(i, perm, input_slot, input_slot_9, input_taint, orig_mem, orig_trace, trace_in, input,
            t0, t1, t2, t3, t4, a, b, c, d, e, f, g, h);
        trace_out := Body_16_XXUnroller(n - 1, i + 1, if perm == 7 then 0 else perm + 1, CheapMod16(i + 1) * 4, CheapMod16(i + 10) * 4,
            input_taint, orig_mem, orig_trace, trace_out, input,
            t0, t1, t3, t2, t4, h, a, b, c, d, e, f, g);
    } else {
        assert OpaqueMod(i, 8) == 0 by { reveal OpaqueMod; }
        trace_out := trace_in;
    }
}

procedure {:refined} Body_16_XXLoopUnrolled(
    ghost trace_in:SHA256Trace,
    ghost input:seq(uint32),
    inline input_taint:taint
    )
    requires Body_16_XXLoopStateInvariantBreakdown(mem, mem, trace_in, trace_in, 16, sp, globals, lr, 
                                                   r2, r12, r3, r1, input_taint, input,
                                                   r4, r5, r6, r7, r8, r9, r10, r11);
    reads  sp; globals;
    modifies mem; lr; r0; r1; r2; r3; r4; r5; r6; r7; r8; r9; r10; r11; r12;
    ensures  exists trace_out :: 
             Body_16_XXLoopStateInvariantBreakdown(old(mem), mem, trace_in, trace_out, 64, sp, globals, lr,
                                                   r2, r12, r3, r1, input_taint, input,
                                                   r4, r5, r6, r7, r8, r9, r10, r11);
{
    assert OpaqueMod(16, 8) == 0 by { reveal OpaqueMod; }
    ghost var trace_out;
    ghost var orig_mem := mem;
    trace_out := Body_16_XXUnroller(48, 16, 0, CheapMod16(16) * 4, CheapMod16(16 + 9) * 4,
        input_taint, orig_mem, trace_in, trace_in, input,
        r0, r2, r12, r3, r1, r4, r5, r6, r7, r8, r9, r10, r11);
}

procedure {:refined} {:timeLimitMultiplier 2} update_Hs(
    ghost base_ptr:uint32,
    inline hash_taint:taint,
    inline input_taint:taint,
    inout t0:opr32,
    inout t1:opr32,
    inout t2:opr32,
    inout t3:opr32,
    inout a:opr32,
    inout b:opr32,
    inout c:opr32,
    inout d:opr32,
    inout e:opr32,
    inout f:opr32,
    inout g:opr32,
    inout h:opr32
    )
requires/ensures
    // Stack is accessible
    ValidAddrs(mem, sp, 19);

    base_ptr == mem[sp + 16*4].v;

    // Base_ptr doesn't alias the stack
    base_ptr + 32 < sp || base_ptr > sp + 19*4;

requires
    ValidSrcAddrs(mem, base_ptr, 8, hash_taint);
    ValidSrcAddr(mem, sp + 16*4, Public);
requires {:refined false}   // Using this style so I can give explicit names to all the registers.  Less confusing this way.
    @t0 == OReg(R0);
    @t1 == OReg(R2);
    @t2 == OReg(R12);
    @t3 == OReg(R3);
    @a  == OReg(R4);
    @b  == OReg(R5);
    @c  == OReg(R6);
    @d  == OReg(R7);
    @e  == OReg(R8);
    @f  == OReg(R9);
    @g  == OReg(R10);
    @h  == OReg(R11);
reads
    sp;
modifies 
    mem; 
ensures
    ValidSrcAddrs(mem, base_ptr, 8, input_taint);

    // Memory framing: We only touch 8 bytes pointed to by the base_ptr
    forall addr:uint32 :: old(mem)?[addr] && (addr < base_ptr || addr >= base_ptr + 8*4)
                    ==> mem?[addr] && mem[addr] == old(mem)[addr];
    mem[base_ptr +  0*4].v == a == BitwiseAdd32(old(mem)[base_ptr +  0*4].v, old(a));
    mem[base_ptr +  1*4].v == b == BitwiseAdd32(old(mem)[base_ptr +  1*4].v, old(b));
    mem[base_ptr +  2*4].v == c == BitwiseAdd32(old(mem)[base_ptr +  2*4].v, old(c));
    mem[base_ptr +  3*4].v == d == BitwiseAdd32(old(mem)[base_ptr +  3*4].v, old(d));
    mem[base_ptr +  4*4].v == e == BitwiseAdd32(old(mem)[base_ptr +  4*4].v, old(e));
    mem[base_ptr +  5*4].v == f == BitwiseAdd32(old(mem)[base_ptr +  5*4].v, old(f));
    mem[base_ptr +  6*4].v == g == BitwiseAdd32(old(mem)[base_ptr +  6*4].v, old(g));
    mem[base_ptr +  7*4].v == h == BitwiseAdd32(old(mem)[base_ptr +  7*4].v, old(h));
{
    lemma_BitwiseAdd32_commutes_forall();
    // Load the ctx pointer that holds the Hs
    rLDR(t3, sp, 64, Public);
    assert t3 == base_ptr;
    rLDR(t0, t3, 0, hash_taint);
    rLDR(t1, t3, 4, hash_taint);
    rLDR(t2, t3, 8, hash_taint);
    rADDWrap(a, a, t0);
    rLDR(t0, t3, 12, hash_taint);
    rADDWrap(b, b, t1);
    rLDR(t1, t3, 16, hash_taint);
    rADDWrap(c, c, t2);
    rLDR(t2,t3,20, hash_taint);
    rADDWrap(d,d,t0);
    rLDR(t0,t3,24, hash_taint);
    rADDWrap(e,e,t1);
    rLDR(t1,t3,28, hash_taint);
    rADDWrap(f,f,t2);
    rADDWrap(g,g,t0);
    rADDWrap(h,h,t1);

    // TODO: OpenSSL does this in a single call to: stmia $t3,{$A,$B,$C,$D,$E,$F,$G,$H}
    rSTR(a, t3,  0, input_taint); 
    rSTR(b, t3,  4, input_taint); 
    rSTR(c, t3,  8, input_taint); 
    rSTR(d, t3, 12, input_taint); 
    rSTR(e, t3, 16, input_taint); 
    rSTR(f, t3, 20, input_taint); 
    rSTR(g, t3, 24, input_taint); 
//    assert { :split_here } true;

    rSTR(h, t3, 28, input_taint); 
}



procedure {:refined} {:timeLimitMultiplier 3} sha256_one_block(
    ghost base_ptr:uint32,
    ghost trace_in:SHA256Trace,
    ghost input:seq(uint32),
    inline hash_taint:taint,
    inline input_taint:taint
    )
requires/ensures
    // Stack is accessible
    ValidAddrs(mem, sp, 19);

    base_ptr == mem[sp + 16*4].v;

    // Base_ptr doesn't alias the stack
    base_ptr + 32 < sp || base_ptr > sp + 19*4;

requires
    // SHA semantics
    IsCompleteSHA256Trace(trace_in);
    SHA256TraceIsCorrect(trace_in);

    // K table adjusted properly
    ValidGlobalsAddr(globals, K_SHA256s().sym, lr);
    globals?[K_SHA256s()];
    AddressOfGlobal(K_SHA256s()) + 256 < 0x1_0000_0000;
    lr == AddressOfGlobal(K_SHA256s());
    SeqLength(globals[K_SHA256s()]) == 256;
    forall j :: 0 <= j < 64 ==> globals[K_SHA256s()][j] == K_SHA256(j);

    ValidSrcAddr(mem, sp + 16*4, Public);

    // Stack slot 16 holds a pointer to a valid, readable region of memory with 8 uint32s of data in it
    ValidSrcAddrs(mem, base_ptr, 8, hash_taint);

    let t0 := r0 in
    let t1 := r2 in
    let t2 := r12 in
    let t3 := r3 in
    let t4 := r1 in
    let a := r4 in
    let b := r5 in
    let c := r6 in
    let d := r7 in
    let e := r8 in
    let f := r9 in
    let g := r10 in
    let h := r11 in
    let input_ptr := r1 in  // Note that this aliases t4
    let ctx := r0 in
    let num_blocks := r2 in // Number of 64-byte blocks to process
        last(trace_in.H)[0] == mem[base_ptr + 0*4].v == a
     && last(trace_in.H)[1] == mem[base_ptr + 1*4].v == b
     && last(trace_in.H)[2] == mem[base_ptr + 2*4].v == c
     && last(trace_in.H)[3] == mem[base_ptr + 3*4].v == d
     && last(trace_in.H)[4] == mem[base_ptr + 4*4].v == e
     && last(trace_in.H)[5] == mem[base_ptr + 5*4].v == f
     && last(trace_in.H)[6] == mem[base_ptr + 6*4].v == g
     && last(trace_in.H)[7] == mem[base_ptr + 7*4].v == h

     // Ghost input matches in-memory input
     && SeqLength(input) == 16
     && (input_ptr + 16*4) < 0x1_0000_0000
     && (input_ptr + 16*4 < sp || sp + 19*4 <= input_ptr)    // input_ptr doesn't alias the stack
     && ValidSrcAddrs(mem, input_ptr, 16, input_taint)
     && (forall j { mem?[input_ptr+j*4] } :: 0 <= j < 16 ==> mem[input_ptr + j*4].v == input[j])
     ;
reads
    sp; globals;
modifies
    mem; r0; r1; r2; r3; r4; r5; r6; r7; r8; r9; r10; r11; r12; lr;
ensures
    // Memory framing: We only touch the stack and 8 bytes pointed to by the base_ptr
    forall addr:uint32 :: old(mem)?[addr] && (addr < sp || addr >= sp + 19*4) 
                                        && (addr < base_ptr || addr >= base_ptr + 8*4)
                    ==> mem?[addr] && mem[addr] == old(mem)[addr];

    lr == AddressOfGlobal(K_SHA256s()) + 256;

    mem[sp + 16*4] == old(mem)[sp + 16*4];
    mem[sp + 17*4].v == old(r1) + 64;
    mem[sp + 17*4].t == Public;
    mem[sp + 18*4] == old(mem)[sp + 18*4];

    // Stack slot 16 holds a pointer to a valid, readable region of memory with 8 uint32s of data in it
    ValidSrcAddrs(mem, base_ptr, 8, input_taint);

    exists trace_out:SHA256Trace ::
           IsCompleteSHA256Trace(trace_out)
        && SHA256TraceIsCorrect(trace_out)
        && trace_out.M == trace_in.M + seq(bswap32_seq(input))
        && mem[base_ptr + 0*4].v ==  r4 == last(trace_out.H)[0]
        && mem[base_ptr + 1*4].v ==  r5 == last(trace_out.H)[1]
        && mem[base_ptr + 2*4].v ==  r6 == last(trace_out.H)[2]
        && mem[base_ptr + 3*4].v ==  r7 == last(trace_out.H)[3]
        && mem[base_ptr + 4*4].v ==  r8 == last(trace_out.H)[4]
        && mem[base_ptr + 5*4].v ==  r9 == last(trace_out.H)[5]
        && mem[base_ptr + 6*4].v == r10 == last(trace_out.H)[6]
        && mem[base_ptr + 7*4].v == r11 == last(trace_out.H)[7];
{
    lemma_BitwiseAdd32_commutes_forall();
    // Prepare the incoming trace by incorporating the input we're about to digest
    ghost var bswapped_input := bswap32_seq(input);
    ghost var new_Ws := ComputeWs(bswapped_input);
    ghost var init_atoh := atoh_c(r4, r5, r6, r7, r8, r9, r10, r11);
    ghost var new_trace_in := lemma_SHA256DigestOneBlockHelper1(trace_in, new_Ws, init_atoh, bswapped_input);

    forall j :| 0 <= j < 16 :: last(new_trace_in.W)[j] == bswap32(input[j])
        { assert TStep(j); }
    assert IsSHA256TraceReadyForStep(new_trace_in, 0);

    // Set up the initial conditions for BODY_00_15
    assert ValidAddr(mem, r1 + 0*4);    // OBSERVE that r1 aka t1 is a ValidAddr
    rLDR(r2, r1, 0, input_taint);        // t1 <- input[0]
    assert r2 == input[0];
    rADDWrap(r1, r1, 4);        // TODO: OpenSSL does this with a single LDR instruction
    rEOR(r3, r5, r6);       // t3 <- B xor C  "@magic"
    //rEOR(r12, r12, r12);    // Note: OpenSSL includes this unnecessarily, since for i=0, we clobber r12
    //assert r12 == 0 by { lemma_BitwiseXorWithItself(r12); }

    assert r1 /*aka t4*/ == old(r1) + 1 * 4 by { reveal BitwiseAdd32; }    // OBSERVE that t4 is a ValidAddr
    ghost var input_ptr := old(r1);      // Avoid Vale mis-capture

    assert Body_00_15LoopStateInvariantBreakdown(mem, mem, input_ptr, input_taint, new_trace_in, new_trace_in, 
                                                 0, sp, globals, lr, 
                                                 r2, r12, r3, r1, input,
                                                 r4, r5, r6, r7, r8, r9, r10, r11);

    ghost var trace_00_15:SHA256Trace;
    trace_00_15 := Body_00_15LoopUnrolled(input_ptr, new_trace_in, input, input_taint);

    assert Body_00_15LoopStateInvariantBreakdown(old(mem), mem, input_ptr, input_taint, new_trace_in, trace_00_15,
                                                 16, sp, globals, lr,
                                                 r2, r12, r3, r1, input,
                                                 r4, r5, r6, r7, r8, r9, r10, r11);
    // Prove ValidSrcAddrs(mem, base_ptr, 8, hash_taint);
    forall addr :| base_ptr <= addr < base_ptr + 8 * 4 && (addr - base_ptr) % 4 == 0 
        :: ValidSrcAddr(mem, addr, hash_taint)
    {
        assert old(mem)?[addr];
    }

    ghost var mid_mem := mem;
    reveal Body_16_XXLoopStateInvariantBreakdown;
    Body_16_XXLoopUnrolled(trace_00_15, input, input_taint);

    exists trace_16_XX :: 
             Body_16_XXLoopStateInvariantBreakdown(mid_mem, mem, trace_00_15, trace_16_XX, 64, sp, globals, lr,
                                                   r2, r12, r3, r1, input_taint, input,
                                                   r4, r5, r6, r7, r8, r9, r10, r11);
    rADDWrap(r4, r4, r12);      // Add final Maj into a

    // Prove ValidSrcAddrs(mem, base_ptr, 8, hash_taint);
    forall addr :| base_ptr <= addr < base_ptr + 8 * 4 && (addr - base_ptr) % 4 == 0 
        :: ValidSrcAddr(mem, addr, hash_taint)
    {
        assert old(mem)?[addr];
    }
    update_Hs(base_ptr, hash_taint, input_taint, r0, r2, r12, r3, r4, r5, r6, r7, r8, r9, r10, r11);

    ghost var old_H := seq( old(mem)[base_ptr +  0].v,
                            old(mem)[base_ptr +  4].v,
                            old(mem)[base_ptr +  8].v,
                            old(mem)[base_ptr + 12].v,
                            old(mem)[base_ptr + 16].v,
                            old(mem)[base_ptr + 20].v,
                            old(mem)[base_ptr + 24].v,
                            old(mem)[base_ptr + 28].v);
    assert mem?[base_ptr];
    ghost var new_H := seq( mem[base_ptr +  0].v,
                            mem[base_ptr +  4].v,
                            mem[base_ptr +  8].v,
                            mem[base_ptr + 12].v,
                            mem[base_ptr + 16].v,
                            mem[base_ptr + 20].v,
                            mem[base_ptr + 24].v,
                            mem[base_ptr + 28].v);
    ghost var trace_out := lemma_SHA256DigestOneBlockHelper2(trace_16_XX, old_H, new_H);
}

procedure {:refined} sha256_loop_body(
    inline input_taint:taint,
    inline hash_taint:taint,
    ghost ctx_ptr:uint32,
    ghost input_ptr:uint32,
    ghost input:seq(uint32),
    ghost num_blocks:nat,
    ghost old_M_length:nat,
    ghost old_mem:memmap,
    ghost block:nat
    )
requires block < num_blocks;
requires exists trace_in ::
         BlockInvariant(trace_in, input, globals, old_M_length, old_mem, mem, sp, lr, r1, r12,
                        r4, r5, r6, r7, r8, r9, r10, r11,
                        input_taint, input_ptr, hash_taint, ctx_ptr, num_blocks, block);
reads
    sp; globals;
modifies
    mem; r0; r1; r2; r3; r4; r5; r6; r7; r8; r9; r10; r11; r12; lr;
ensures exists trace_out ::
        BlockInvariant(trace_out, input, globals, old_M_length, old_mem, mem, sp, lr, r1, r12,
                        r4, r5, r6, r7, r8, r9, r10, r11,
                        input_taint, input_ptr, hash_taint, ctx_ptr, num_blocks, block+1);
{
    ghost var block_input := SeqSlice(input, block*16, (block+1)*16);

    ghost var current_input_ptr := r1;  // == input_ptr + block*16*4
    forall j :| 0 <= j < 16 :: ValidAddr(mem, current_input_ptr + j*4)
                            && mem[current_input_ptr + j*4].v == block_input[j]
    {
        assert current_input_ptr + j*4 == input_ptr + (block*16+j)*4;
        assert ValidAddr(mem, current_input_ptr + j*4);
    }
    ghost var prev_mem := mem;
    exists trace_in ::
         BlockInvariant(trace_in, input, globals, old_M_length, mem, mem, sp, lr, r1, r12,
                        r4, r5, r6, r7, r8, r9, r10, r11,
                        input_taint, input_ptr, hash_taint, ctx_ptr, num_blocks, block);
    sha256_one_block(ctx_ptr, trace_in, block_input, hash_taint, input_taint);
    exists trace_out:SHA256Trace ::
           IsCompleteSHA256Trace(trace_out)
        && SHA256TraceIsCorrect(trace_out)
        && trace_out.M == trace_in.M + seq(bswap32_seq(block_input))
        && mem[ctx_ptr + 0*4].v ==  r4 == last(trace_out.H)[0]
        && mem[ctx_ptr + 1*4].v ==  r5 == last(trace_out.H)[1]
        && mem[ctx_ptr + 2*4].v ==  r6 == last(trace_out.H)[2]
        && mem[ctx_ptr + 3*4].v ==  r7 == last(trace_out.H)[3]
        && mem[ctx_ptr + 4*4].v ==  r8 == last(trace_out.H)[4]
        && mem[ctx_ptr + 5*4].v ==  r9 == last(trace_out.H)[5]
        && mem[ctx_ptr + 6*4].v == r10 == last(trace_out.H)[6]
        && mem[ctx_ptr + 7*4].v == r11 == last(trace_out.H)[7];

    // TODO: The first time we come through, block invariant says:
    //  (block > 0 ==> ValidSrcAddr(mem, sp + 68, Public)) 
    // but sha_256_one_block doesn't tell us that it set it to Public
    assert 68 == 17*4;
    rLDR(r1, sp, 68, Public);  // Reload input_ptr
    assert 72 == 18*4; 
    rLDR(r12, sp, 72, Public); // Reload end_ptr
    rSUB(lr, lr, 256); // Reset lr

    forall addr :| input_ptr <= addr < input_ptr + (num_blocks*16)*4 && (addr - input_ptr) % 4 == 0
        :: ValidSrcAddr(mem, addr, input_taint)
    {
        assert ValidSrcAddr(old(mem), addr, input_taint);
    }
    assert ValidSrcAddrs(mem, input_ptr, num_blocks * 16, input_taint);

    forall j :| 0 <= j < num_blocks * 16 :: mem[input_ptr + j*4].v == input[j]
    {
        assert old(mem)?[input_ptr + j*4];
    }

    assert BlockInvariant(trace_out, input, globals, old_M_length, old_mem, mem, sp, lr, r1, r12,
                          r4, r5, r6, r7, r8, r9, r10, r11,
                          input_taint, input_ptr, hash_taint, ctx_ptr, num_blocks, block+1);
}


procedure {:refined} sha256_loop(
    inline input_taint:taint,
    inline hash_taint:taint,
    ghost ctx_ptr:uint32,
    ghost input_ptr:uint32,
    ghost input:seq(uint32),
    ghost num_blocks:nat,
    ghost old_M_length:nat,
    ghost old_mem:memmap
    )
requires exists trace_in ::
         BlockInvariant(trace_in, input, globals, old_M_length, old_mem, mem, sp, lr, r1, r12,
                        r4, r5, r6, r7, r8, r9, r10, r11,
                        input_taint, input_ptr, hash_taint, ctx_ptr, num_blocks, 0);
reads
    sp; globals;
modifies
    mem; r0; r1; r2; r3; r4; r5; r6; r7; r8; r9; r10; r11; r12; lr;
ensures exists trace_out ::
        BlockInvariant(trace_out, input, globals, old_M_length, old_mem, mem, sp, lr, r1, r12,
                        r4, r5, r6, r7, r8, r9, r10, r11,
                        input_taint, input_ptr, hash_taint, ctx_ptr, num_blocks, num_blocks);
{
    ghost var block:nat := 0;
    while (r1 < r12)
        invariant
            exists trace :: 
                BlockInvariant(trace, input, globals, old_M_length, old_mem, mem, sp, lr, r1, r12,
                               r4, r5, r6, r7, r8, r9, r10, r11,
                               input_taint, input_ptr, hash_taint, ctx_ptr, num_blocks, block);
         decreases r12 - r1;
    {
        sha256_loop_body(input_taint, hash_taint, ctx_ptr, input_ptr, input, num_blocks, 
                         old_M_length, old_mem, block);
        block := block + 1;
    }
}

// Core implementation that does the real work
procedure {:refined} {:timeLimitMultiplier 2} sha256_block_data_order_inner(
    inline input_taint:taint,
    inline hash_taint:taint,
    ghost trace_in:SHA256Trace,
    ghost input:seq(uint32)
    )
requires/ensures
    // Stack is accessible
    ValidAddrs(mem, sp, 19);

requires
    hash_taint == input_taint;      // Really, hash_taint >= input_taint, so this is conservative
    IsCompleteSHA256Trace(trace_in);
    SHA256TraceIsCorrect(trace_in);

    // K table is valid
    ValidGlobals(globals);
    ValidGlobal(K_SHA256s());
    SeqLength(globals[K_SHA256s()]) == 256;
    AddressOfGlobal(K_SHA256s()) + 256 < 0x1_0000_0000;
    forall j :: 0 <= j < 64 ==> globals[K_SHA256s()][j] == K_SHA256(j);

    let ctx := r0 in
    let input_ptr := r1 in
    let num_blocks := r2 in // Number of 64-byte blocks to process

     // Old H values are laid out in memory pointed at by ctx
        ValidSrcAddrs(mem, ctx, 8, hash_taint) 
     && last(trace_in.H)[0] == mem[ctx + 0*4].v
     && last(trace_in.H)[1] == mem[ctx + 1*4].v
     && last(trace_in.H)[2] == mem[ctx + 2*4].v
     && last(trace_in.H)[3] == mem[ctx + 3*4].v
     && last(trace_in.H)[4] == mem[ctx + 4*4].v
     && last(trace_in.H)[5] == mem[ctx + 5*4].v
     && last(trace_in.H)[6] == mem[ctx + 6*4].v
     && last(trace_in.H)[7] == mem[ctx + 7*4].v

     // Ghost input matches in-memory input
     && SeqLength(input) == num_blocks*16
     && input_ptr + num_blocks*16*4 < 0x1_0000_0000
     && ValidSrcAddrs(mem, input_ptr, num_blocks*16, input_taint)
     && (forall j { mem?[input_ptr+j*4] } :: 0 <= j < num_blocks*16 ==> mem[input_ptr + j*4].v == input[j])

     // Anti-aliasing
     && (ctx + 32 < input_ptr || ctx > input_ptr + num_blocks*16*4)    // input_ptr != ctx
     && (ctx + 32 < sp || ctx > sp + 19*4)                             // ctx != sp
     && (input_ptr + num_blocks*16*4 < sp || input_ptr >= sp + 19*4);  // input_ptr != sp

reads
    sp; globals;
modifies
    mem; r0; r1; r2; r3; r4; r5; r6; r7; r8; r9; r10; r11; r12; lr;
ensures
    // Memory framing:  We only touch the stack and 8 bytes pointed to by ctx_ptr
    forall addr:uint32 :: old(mem)?[addr] && (addr < sp || addr >= sp + 19 * 4) 
                                        && (addr < old(r0) || addr >= old(r0) + 8 * 4) 
                            ==> mem?[addr] && old(mem)[addr] == mem[addr];
    forall j {ValidAddr(mem, old(r0)+j*4)} { mem?[old(r0)+j*4] } :: 0 <= j < 8 ==> ValidAddr(mem, old(r0) + j*4);

    exists trace_out ::
           IsCompleteSHA256Trace(trace_out)
        && SHA256TraceIsCorrect(trace_out)
        && SeqLength(trace_out.M) == SeqLength(trace_in.M) + old(r2)
        && (forall i :: 0 <= i < old(r2) 
             ==> trace_out.M[SeqLength(trace_in.M) + i] == bswap32_seq(SeqSlice(input, i*16, (i+1)*16))) 
        && last(trace_out.H)[0] == mem[old(r0) + 0*4].v
        && last(trace_out.H)[1] == mem[old(r0) + 1*4].v
        && last(trace_out.H)[2] == mem[old(r0) + 2*4].v
        && last(trace_out.H)[3] == mem[old(r0) + 3*4].v
        && last(trace_out.H)[4] == mem[old(r0) + 4*4].v
        && last(trace_out.H)[5] == mem[old(r0) + 5*4].v
        && last(trace_out.H)[6] == mem[old(r0) + 6*4].v
        && last(trace_out.H)[7] == mem[old(r0) + 7*4].v
        ;

{
    rLDRglobaladdr(lr, K_SHA256s().sym);

    ghost var ctx_ptr := r0;
    ghost var input_ptr := r1;
    ghost var num_blocks := r2;

    rADDWrapShift(r2, r1, r2, LSLShift(6)); // r2 <- input_ptr + 64 * num_blocks
    assert r2 == r1 + 64 * num_blocks by {
        reveal BitwiseAdd32;
        lemma_ShiftsAdd(num_blocks, 2, 4);
        lemma_LeftShift2(num_blocks);
        lemma_LeftShift4(num_blocks*4);
    }

    // Save some of the initial state away.  We'll need it later.
    assert 64 == 16*4;
    rSTR(r0, sp, 64, Public);
    assert 72 == 18*4;
    assert ValidAddr(old(mem), sp + 18*4);      // OBSERVE
    rSTR(r2, sp, 72, Public);
    // Move end_ptr into r12 to satisfy the BlockInvariant. 
    // Note: OpenSSL appears to avoid this by assuming num_blocks > 0
    rMOV(r12, r2);

    // Lots of OBSERVE
    assert ValidAddr(old(mem), r0 + 0*4);
    assert ValidAddr(old(mem), r0 + 1*4);
    assert ValidAddr(old(mem), r0 + 2*4);
    assert ValidAddr(old(mem), r0 + 3*4);
    assert ValidAddr(old(mem), r0 + 4*4);
    assert ValidAddr(old(mem), r0 + 5*4);
    assert ValidAddr(old(mem), r0 + 6*4);
    assert ValidAddr(old(mem), r0 + 7*4);
    // Load a - h values into registers
    rLDR(r4,  r0,  0, hash_taint);
    rLDR(r5,  r0,  4, hash_taint);
    rLDR(r6,  r0,  8, hash_taint);
    rLDR(r7,  r0, 12, hash_taint);
    rLDR(r8,  r0, 16, hash_taint);
    rLDR(r9,  r0, 20, hash_taint);
    rLDR(r10, r0, 24, hash_taint);
    rLDR(r11, r0, 28, hash_taint);

    forall j :| 0 <= j < 19 :: ValidAddr(mem, sp + j*4)
    {
        assert ValidAddr(old(mem), sp + j*4);
    }

    forall addr :| input_ptr <= addr < input_ptr + (num_blocks*16)*4 && (addr - input_ptr) % 4 == 0
        :: ValidSrcAddr(mem, addr, input_taint)
    {
        assert ValidSrcAddr(old(mem), addr, input_taint);
    }
    assert ValidSrcAddrs(mem, input_ptr, num_blocks * 16, input_taint);

    forall j :| 0 <= j < num_blocks * 16 :: mem[input_ptr + j*4].v == input[j]
    {
        assert ValidAddr(old(mem), input_ptr + j*4);
    }

    // OBSERVE
    ghost var prev_mem := mem;  // Avoid Vale mis-capture
    ghost var len_M := SeqLength(trace_in.M); 
    assert BlockInvariant(trace_in, input, globals, len_M, prev_mem, mem, sp, lr, r1, r12,
                          r4, r5, r6, r7, r8, r9, r10, r11,
                          input_taint, input_ptr, hash_taint, ctx_ptr, num_blocks, 0);
    sha256_loop(input_taint, hash_taint, ctx_ptr, input_ptr, input, num_blocks, len_M, prev_mem);
    exists trace_out ::
        BlockInvariant(trace_out, input, globals, len_M, prev_mem, mem, sp, lr, r1, r12,
                        r4, r5, r6, r7, r8, r9, r10, r11,
                        input_taint, input_ptr, hash_taint, ctx_ptr, num_blocks, num_blocks);
    assert IsCompleteSHA256Trace(trace_out);
}

procedure {:refined} scrub_stack()
requires/ensures
    ValidAddrs(mem, sp, 29);  // Stack is accessible
reads r0; sp;
modifies mem;
ensures
    ValidSrcAddrs(mem, sp, 29, Public);  // Stack is publicly readable
    forall addr :: old(mem)?[addr] && (addr < sp || addr >= sp + 116) 
                ==> mem?[addr] && old(mem)[addr] == mem[addr];
{
    rSTR(r0, sp, 0, Public);
    rSTR(r0, sp, 4, Public);
    rSTR(r0, sp, 8, Public);
    rSTR(r0, sp, 12, Public);
    rSTR(r0, sp, 16, Public);
    rSTR(r0, sp, 20, Public);
    rSTR(r0, sp, 24, Public);
    rSTR(r0, sp, 28, Public);
    rSTR(r0, sp, 32, Public);
    rSTR(r0, sp, 36, Public);
    rSTR(r0, sp, 40, Public);
    rSTR(r0, sp, 44, Public);
    rSTR(r0, sp, 48, Public);
    rSTR(r0, sp, 52, Public);
    rSTR(r0, sp, 56, Public);
    rSTR(r0, sp, 60, Public);
    rSTR(r0, sp, 64, Public);
    rSTR(r0, sp, 68, Public);
    rSTR(r0, sp, 72, Public);
    rSTR(r0, sp, 76, Public);
    rSTR(r0, sp, 80, Public);
    rSTR(r0, sp, 84, Public);
    rSTR(r0, sp, 88, Public);
    rSTR(r0, sp, 92, Public);
    rSTR(r0, sp, 96, Public);
    rSTR(r0, sp, 100, Public);
    rSTR(r0, sp, 104, Public);
    rSTR(r0, sp, 108, Public);
    rSTR(r0, sp, 112, Public);
    assert ValidSrcAddrs(mem, sp, 29, Public);
}

// Wrapper around the inner version that conforms to the calling convention
procedure {:refined} {:timeLimitMultiplier 3} sha256_block_data_order(
    inline input_taint:taint,
    inline hash_taint:taint,
    ghost trace_in:SHA256Trace,
    ghost input:seq(uint32)
    )
requires/ensures
    // Stack is accessible
    ValidAddrs(mem, sp-116, 29);

requires
    hash_taint == input_taint;      // Really, hash_taint >= input_taint, so this is conservative
    IsCompleteSHA256Trace(trace_in);
    SHA256TraceIsCorrect(trace_in);

    sp >= 116;

    // K table is valid
    ValidGlobals(globals);
    ValidGlobal(K_SHA256s());
    SeqLength(globals[K_SHA256s()]) == 256;
    AddressOfGlobal(K_SHA256s()) + 256 < 0x1_0000_0000;
    forall j :: 0 <= j < 64 ==> globals[K_SHA256s()][j] == K_SHA256(j);

    let ctx := r0 in
    let input_ptr := r1 in
    let num_blocks := r2 in // Number of 64-byte blocks to process

     // Old H values are laid out in memory pointed at by ctx
        ValidSrcAddrs(mem, ctx, 8, hash_taint) 
     && last(trace_in.H)[0] == mem[ctx + 0*4].v
     && last(trace_in.H)[1] == mem[ctx + 1*4].v
     && last(trace_in.H)[2] == mem[ctx + 2*4].v
     && last(trace_in.H)[3] == mem[ctx + 3*4].v
     && last(trace_in.H)[4] == mem[ctx + 4*4].v
     && last(trace_in.H)[5] == mem[ctx + 5*4].v
     && last(trace_in.H)[6] == mem[ctx + 6*4].v
     && last(trace_in.H)[7] == mem[ctx + 7*4].v

     // Ghost input matches in-memory input
     && SeqLength(input) == num_blocks*16
     && input_ptr + num_blocks*16*4 < 0x1_0000_0000
     && ValidSrcAddrs(mem, input_ptr, num_blocks*16, input_taint)
     && (forall j { mem?[input_ptr+j*4] } :: 0 <= j < num_blocks*16 ==> mem[input_ptr + j*4].v == input[j])

     // Anti-aliasing
     && (ctx + 32 < input_ptr || ctx > input_ptr + num_blocks*16*4)    // input_ptr != ctx
     && (ctx + 32 < sp - 116 || ctx > sp)                              // ctx != sp
     && (input_ptr + num_blocks*16*4 < sp - 116 || input_ptr >= sp);   // input_ptr != sp

reads
    globals;
modifies
    mem; r0; r1; r2; r3; r4; r5; r6; r7; r8; r9; r10; r11; r12; sp; lr;
ensures
    // Memory framing:  We only touch the stack and 8 bytes pointed to by ctx_ptr
    forall addr:uint32 :: old(mem)?[addr] && (addr < sp - 116 || addr >= sp) 
                                        && (addr < old(r0) || addr >= old(r0) + 8 * 4) 
                            ==> mem?[addr] && old(mem)[addr] == mem[addr];
    ValidAddrs(mem, old(r0), 8);
    ValidSrcAddrs(mem, sp-116, 29, Public);  // Stack is publicly readable

    // Calling convention
    r4 == old(r4);
    r5 == old(r5);
    r6 == old(r6);
    r7 == old(r7);
    r8 == old(r8);
    r9 == old(r9);
    r10== old(r10);
    r11== old(r11);
    sp == old(sp);
    lr == old(lr);

    // SHA results
    exists trace_out ::
           IsCompleteSHA256Trace(trace_out)
        && SHA256TraceIsCorrect(trace_out)
        && SeqLength(trace_out.M) == SeqLength(trace_in.M) + old(r2)
        && (forall i :: 0 <= i < old(r2) 
             ==> trace_out.M[SeqLength(trace_in.M) + i] == bswap32_seq(SeqSlice(input, i*16, (i+1)*16))) 
        && last(trace_out.H)[0] == mem[old(r0) + 0*4].v
        && last(trace_out.H)[1] == mem[old(r0) + 1*4].v
        && last(trace_out.H)[2] == mem[old(r0) + 2*4].v
        && last(trace_out.H)[3] == mem[old(r0) + 3*4].v
        && last(trace_out.H)[4] == mem[old(r0) + 4*4].v
        && last(trace_out.H)[5] == mem[old(r0) + 5*4].v
        && last(trace_out.H)[6] == mem[old(r0) + 6*4].v
        && last(trace_out.H)[7] == mem[old(r0) + 7*4].v
        ;
{
    // We need 10 slots to save/restore registers, and then the inner routine needs 19 slots for scratch space
    rSUB(sp, sp, 116);

    // Save nonvolatile registers
    assert 112 == 28*4;
    rSTR(lr,  sp, 112, Secret);
    assert 108 == 27*4;
    assert ValidAddr(old(mem), old(sp) - 116 + 27*4);
    rSTR(r4,  sp, 108, Secret);
    assert 104 == 26*4;
    assert ValidAddr(old(mem), old(sp) - 116 + 26*4);
    rSTR(r5,  sp, 104, Secret);
    //assert 100 == 25*4;
    assert ValidAddr(old(mem), old(sp) - 116 + 25*4);
    rSTR(r6,  sp, 100, Secret);
    //assert 96 == 24*4;
    assert ValidAddr(old(mem), old(sp) - 116 + 24*4);
    rSTR(r7,  sp, 96, Secret);
    //assert 92 == 23*4;
    assert ValidAddr(old(mem), old(sp) - 116 + 23*4);
    rSTR(r8,  sp, 92, Secret);
    //assert 88 == 22*4;
    assert ValidAddr(old(mem), old(sp) - 116 + 22*4);
    rSTR(r9,  sp, 88, Secret);
    //assert 84 == 21*4;
    assert ValidAddr(old(mem), old(sp) - 116 + 21*4);
    rSTR(r10, sp, 84, Secret);
    //assert 80 == 20*4;
    assert ValidAddr(old(mem), old(sp) - 116 + 20*4);
    rSTR(r11, sp, 80, Secret);
    //assert 76 == 19*4;
    assert ValidAddr(old(mem), old(sp) - 116 + 19*4);
    rSTR(r12, sp, 76, Secret);


    // Prove the stack is accessible (sigh)
    forall j {ValidAddr(mem, sp+j*4)} { mem?[sp+j*4] } :| 0 <= j < 19 :: ValidAddr(mem, sp + j*4)
    {
        assert ValidAddr(old(mem), old(sp) - 116 + j*4);
    }

    // Prove that ctx is still accessible (sigh)
    forall addr :| r0 <= addr < r0 + 8 * 4 && (addr - r0) % 4 == 0 
        :: ValidSrcAddr(mem, addr, hash_taint)
    {
        assert old(mem)?[addr];
    }

    // Prove that input is still accessible (sigh)
    ghost var num_blocks := r2;
    forall addr :| r1 <= addr < r1 + (num_blocks*16) * 4 && (addr - r1) % 4 == 0 
        :: ValidSrcAddr(mem, addr, hash_taint)
    {
        assert old(mem)?[addr];
    }

    sha256_block_data_order_inner(input_taint, hash_taint, trace_in, input);

    // Restore the nonvolatile registers
    rLDR(lr,  sp, 112, Secret);
    rLDR(r4,  sp, 108, Secret);
    rLDR(r5,  sp, 104, Secret);
    rLDR(r6,  sp, 100, Secret);
    rLDR(r7,  sp, 96, Secret);
    rLDR(r8,  sp, 92, Secret);
    rLDR(r9,  sp, 88, Secret);
    rLDR(r10, sp, 84, Secret);
    rLDR(r11, sp, 80, Secret);
    rLDR(r12, sp, 76, Secret);

    rMOV(r0, 0);
    rMOV(r2, 0);
    rMOV(r3, 0);

    scrub_stack();

    // Restore the stack pointer
    rADD(sp, sp, 116);
}

#verbatim

method Main()
    decreases *
{
    var code := va_code_sha256_block_data_order(Secret, Secret);
    var ts:taintState := TaintState(map[R0 := Public, R1 := Public, R2 := Public, SP := Public]);
    var b := checkConstantTime("sha256_block_data_order", code, ts);

    if (b == false) {
        print ("Constant time check failed");
    } else {
        var tsExpected:taintState := TaintState(map[R0 := Public, R1 := Public, R2 := Public, R3 := Public]);
        b := checkLeakage("sha256_block_data_order", code, ts, tsExpected);

        if (b == false) {
            print ("Leakage check failed");
        } else {
            printHeader();
            var n := printFunction("sha256_block_data_order", code, 0);
            print "  BX lr"; nl();
            printBss(KomGlobalDecls());
            printFooter();
        }
    }
}
} // end module sha256_vale

#endverbatim
