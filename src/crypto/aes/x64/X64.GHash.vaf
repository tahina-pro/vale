include "../../../arch/x64/X64.Vale.InsBasic.vaf"
include "../../../arch/x64/X64.Vale.InsMem.vaf"
include "../../../arch/x64/X64.Vale.InsVector.vaf"
include "X64.GF128_Mul.vaf"

module X64.GHash

#verbatim{:interface}{:implementation}
open Opaque_s
open FStar.Seq
open Words_s
open Types_s
open Types_i
open AES_s
open GHash_s
open GHash_i
open GF128_s
open GF128_i
open GCTR_s
open GCM_helpers_i
open Math.Poly2_s
open X64.Poly1305.Math_i
open X64.GF128_Mul
open X64.Machine_s
open X64.Memory_i
open X64.Vale.State_i
open X64.Vale.Decls_i
open X64.Vale.InsBasic
open X64.Vale.InsMem
open X64.Vale.InsVector
open X64.Vale.InsAes
open X64.Vale.QuickCode_i
open X64.Vale.QuickCodes_i
#endverbatim

#verbatim{:interface}
let get_last_slice_workaround (s:seq quad32) (start_pos end_pos:int)  =
  if 0 <= start_pos && start_pos < end_pos && end_pos <= length s then
    last (slice s start_pos end_pos)
  else
    Mkfour 0 0 0 0 

//let slice_workaround (s:seq quad32) (start_pos end_pos:int)  =
//  if 0 <= start_pos && start_pos < end_pos && end_pos <= length s then
//    slice s start_pos end_pos
//  else
//    create 1 (Mkfour 0 0 0 0)
#endverbatim

#reset-options "--z3rlimit 30"

///////////////////////////
// GHash
///////////////////////////

procedure{:quick} ReduceMul128_LE(ghost a:poly, ghost b:poly)
    lets mask @= xmm8;
    reads mask;
    modifies
        efl; r12;
        xmm1; xmm2; xmm3; xmm4; xmm5; xmm6;
    requires
        degree(a) <= 127;
        degree(b) <= 127;
        xmm1 == reverse_bytes_quad32(gf128_to_quad32(a));
        xmm2 == reverse_bytes_quad32(gf128_to_quad32(b));
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
    ensures
        xmm1 == reverse_bytes_quad32(gf128_to_quad32(gf128_mul(a, b)));
{
    Pshufb(xmm1, mask);
    Pshufb(xmm2, mask);
    ReduceMulRev128(a, b);
    Pshufb(xmm1, mask);
}

procedure {:quick} compute_Y0()
    modifies xmm1; efl;
    ensures xmm1 == Mkfour(0, 0, 0, 0);
{
    Pxor(xmm1, xmm1);
    lemma_quad32_xor();
}

procedure {:quick} compute_ghash_incremental_register()
    lets input @= xmm2; io @= xmm1; mask @= xmm8; h @= xmm11;
    requires mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
    reads
        h; mask;
    modifies
        io; efl; r12;
        xmm2; xmm3; xmm4; xmm5; xmm6;
    ensures
        io == ghash_incremental(h, old(io), create(1, old(input)));
{
    Pxor(io, input);    // Y_i := Y_{i-1} ^ x_i
    Mov128(xmm2, h);    // Move h into the register that ReduceMul128_LE expects

    lemma_to_of_quad32(reverse_bytes_quad32(io));   // Help satisfy precondition
    lemma_to_of_quad32(reverse_bytes_quad32(h));    // Help satisfy precondition
    ReduceMul128_LE(gf128_of_quad32(reverse_bytes_quad32(io)), gf128_of_quad32(reverse_bytes_quad32(h)));    // io := Y_i * H
    reveal_opaque(ghash_incremental_def);
}

procedure {:quick} compute_ghash_incremental(
    ghost in_b:buffer128
    )
    lets in_ptr @= rax; len @= rcx; io @= xmm1; mask @= xmm8; h @= xmm11; 
    reads
        in_ptr; len; h; mask;
        mem;

    modifies
        rdx; r9; r12;
        io; efl;

        // ReduceMul128_LE touches almost everything
        xmm2; xmm3; xmm4; xmm5; xmm6;

    requires
        // GHash reqs
        len > 0 ==> validSrcAddrs128(mem, in_ptr, in_b, len);
        len > 0 ==> in_ptr  + 16 * len < pow2_64;
        len > 0 ==> buffer_length(in_b) == len;
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

    ensures
        len == 0 ==> rdx == old(rdx) /\ r9 == old(r9) /\ xmm1 == old(xmm1) /\ io == old(io);
        len > 0 ==> length(buffer128_as_seq(mem, in_b)) > 0 /\ io == ghash_incremental(h, old(io), buffer128_as_seq(mem, in_b));
{
    if (len != 0) {
        Mov64(rdx, 0);
        Mov64(r9, in_ptr);

        while (rdx != len)
            invariant
                //////////////////// Basic indexing //////////////////////
                0 <= rdx <= len;
                r9 == in_ptr + 16 * rdx;
                
                //////////////////// From requires //////////////////////
                // GHash reqs
                validSrcAddrs128(mem, in_ptr, in_b, len);
                in_ptr  + 16 * len < pow2_64;
                len > 0;
                buffer_length(in_b) == len;
                mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

                //////////////////// Postcondition goals //////////////////////
                rdx == 0 ==> io == old(io);
                rdx > 0 ==> io == ghash_incremental(h, old(io), slice_work_around(buffer128_as_seq(mem, in_b), rdx));

            decreases
                len - rdx;
        {
            Load128_buffer(xmm2, r9, 0, in_b, rdx);
            Pxor(io, xmm2);        // Y_i := Y_{i-1} ^ x_i
            Mov128(xmm2, h);       // xmm2 := H

            lemma_to_of_quad32(reverse_bytes_quad32(xmm1));   // Help satisfy precondition
            lemma_to_of_quad32(reverse_bytes_quad32(xmm2));   // Help satisfy precondition
            ReduceMul128_LE(gf128_of_quad32(reverse_bytes_quad32(xmm1)), gf128_of_quad32(reverse_bytes_quad32(xmm2)));  // io := Y_i * H

            Add64(rdx, 1);
            Add64(r9, 16);
            reveal_opaque(ghash_incremental_def);
        }
    }
}

procedure {:quick} compute_ghash_incremental_partial(
    ghost in_b:buffer128
    )
    lets in_ptr @= rax; len @= rcx; io @= xmm1; mask @= xmm8; h @= xmm11; 
    reads
        in_ptr; len; h; mask;
        mem;

    modifies
        rdx; r9; r12;
        io; efl;

        // ReduceMul128_LE touches almost everything
        xmm2; xmm3; xmm4; xmm5; xmm6;

    requires
        // GHash reqs
        len > 0 ==> validSrcAddrs128(mem, in_ptr, in_b, len);
        len > 0 ==> in_ptr  + 16 * len < pow2_64;
        len > 0 ==> buffer_length(in_b) >= len;
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

    ensures
        len == 0 ==> rdx == old(rdx) /\ r9 == in_ptr /\ xmm1 == old(xmm1) /\ io == old(io);
        let input := slice_work_around(buffer128_as_seq(mem, in_b), len);
        len > 0 ==> length(input) > 0 /\ io == ghash_incremental(h, old(io), input);
        r9 == in_ptr + 16 * len;
{
    Mov64(r9, in_ptr);
    if (len != 0) {
        Mov64(rdx, 0);

        while (rdx != len)
            invariant
                //////////////////// Basic indexing //////////////////////
                0 <= rdx <= len;
                r9 == in_ptr + 16 * rdx;
                
                //////////////////// From requires //////////////////////
                // GHash reqs
                validSrcAddrs128(mem, in_ptr, in_b, len);
                in_ptr  + 16 * len < pow2_64;
                len > 0;
                buffer_length(in_b) >= len;
                mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

                //////////////////// Postcondition goals //////////////////////
                rdx == 0 ==> io == old(io);
                rdx > 0 ==> io == ghash_incremental(h, old(io), slice_work_around(buffer128_as_seq(mem, in_b), rdx));

            decreases
                len - rdx;
        {
            Load128_buffer(xmm2, r9, 0, in_b, rdx);
            Pxor(io, xmm2);        // Y_i := Y_{i-1} ^ x_i
            Mov128(xmm2, h);       // xmm2 := H

            lemma_to_of_quad32(reverse_bytes_quad32(xmm1));   // Help satisfy precondition
            lemma_to_of_quad32(reverse_bytes_quad32(xmm2));   // Help satisfy precondition
            ReduceMul128_LE(gf128_of_quad32(reverse_bytes_quad32(xmm1)), gf128_of_quad32(reverse_bytes_quad32(xmm2)));  // io := Y_i * H

            Add64(rdx, 1);
            Add64(r9, 16);
            reveal_opaque(ghash_incremental_def);
        }
    }
}

#reset-options "--z3rlimit 10"
// Purely proof work to show that when there are no extra bytes, there's no work left to do
procedure {:quick exportOnly} ghash_incremental_bytes_no_extra(
    ghost in_b:buffer128,
    ghost old_io:quad32,
    ghost old_in_ptr:nat64,
    ghost num_bytes:nat64,
    ghost io:quad32,
    ghost h:quad32
    )
    reads mem;
    requires
        // GHash reqs
        num_bytes > 0 ==> validSrcAddrs128(mem, old_in_ptr, in_b, bytes_to_quad_size(num_bytes));
        num_bytes > 0 ==> old_in_ptr  + 16 * bytes_to_quad_size(num_bytes) < pow2_64;
        buffer_length(in_b) == bytes_to_quad_size(num_bytes);

        //len == 0 ==> rdx == old(rdx) /\ r9 == old(r9) /\ xmm1 == old(xmm1) /\ io == old(io);
        let input := slice_work_around(buffer128_as_seq(mem, in_b), num_bytes / 16);
        num_bytes > 0 ==> length(input) > 0 /\ io == ghash_incremental(h, old_io, input);

        // Extra reqs
        num_bytes % 16 == 0;
    ensures
        let input_bytes := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem, in_b)), num_bytes);
        let padded_bytes := pad_to_128_bits(input_bytes);
        let input_quads := le_bytes_to_seq_quad32(padded_bytes);
        num_bytes > 0 ==> length(input_quads) > 0 /\ 
                          io == ghash_incremental(h, old_io, input_quads);
{
    ghost var num_blocks := num_bytes / 16;

    // Precondition variables
    ghost var input := slice_work_around(buffer128_as_seq(mem, in_b), num_blocks);

    // Postcondition variables
    ghost var input_bytes := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem, in_b)), num_bytes);
    ghost var padded_bytes := pad_to_128_bits(input_bytes);
    ghost var input_quads := le_bytes_to_seq_quad32(padded_bytes);

    // We want to show: input_quads == input

    no_extra_bytes_helper(buffer128_as_seq(mem, in_b), num_bytes);
        // ==> input_bytes == slice (le_seq_quad32_to_bytes in_b) 0 num_bytes == le_seq_quad32_to_bytes in_b
    assert input_bytes == le_seq_quad32_to_bytes(buffer128_as_seq(mem, in_b));
    assert pad_to_128_bits(input_bytes) == input_bytes;
    assert input_quads == le_bytes_to_seq_quad32(input_bytes);

    //slice_commutes_le_seq_quad32_to_bytes0(buffer128_as_seq(mem, in_b), num_blocks);

//    input_quads == le_bytes_to_seq_quad32(le_seq_quad32_to_bytes(buffer128_as_seq(mem, in_b)))
    //le_bytes_to_seq_quad32_to_bytes(slice_work_around(buffer128_as_seq(mem, in_b), num_blocks));
    le_bytes_to_seq_quad32_to_bytes(buffer128_as_seq(mem, in_b));
//    input_quads == buffer128_as_seq(mem, in_b)
//
    assert input == buffer128_as_seq(mem, in_b);
}


/*   
 * Note: More efficient options would be:
 *  a) Write a routine to build 16 masks for Pshufb, each of which implements a shift by 0-16 bytes. 
 *     Ask GCM caller to pass in that buffer.  
 *  b) Write a 16-way switch statement using Psrldq with immediates
 *     Note that a switch statement will likely be much more efficient than a 16-way if/else,
 *     but Vale currently doesn't support switch statements
 */ 
#reset-options "--z3rlimit 30"
procedure {:quick} compute_pad_to_128_bits()
    lets io @= xmm2; num_bytes @= rax; tmp @= rcx; mask @= rdx;

    reads num_bytes;
    modifies io; tmp; mask; efl;
    requires 0 < num_bytes < 16;
    ensures 
        let padded_bytes := pad_to_128_bits(slice_work_around(le_quad32_to_bytes(old(io)), num_bytes));
        length(padded_bytes) = 16 && io = le_bytes_to_quad32(padded_bytes);
{
    lemma_poly_bits64();
    if (num_bytes < 8) {
        // Zero out the top 64-bits
        PinsrqImm(io, 0, 1, tmp);

        // Grab the lower 64 bits and zero-out 1-7 of the bytes
        Mov64(tmp, num_bytes);
        Shl64(tmp, 3);      // tmp == 8 (bits/byte) * num_bytes 
        lemma_bytes_shift_power2(num_bytes); // ==>
        assert tmp == 8 * num_bytes;
        Mov64(mask, 1);
        Shl64(mask, tmp);
        Sub64(mask, 1);
        Pextrq(tmp, io, 0);     
        ghost var old_lower128 := tmp;
        And64(tmp, mask);
        lemma_bytes_and_mod(old_lower128, num_bytes); // ==>
        assert tmp == old_lower128 % (pow2(num_bytes * 8));

        // Restore the lower 64 bits
        Pinsrq(io, tmp, 0);

        pad_to_128_bits_lower(old(io), num_bytes);
    } else {
        assert num_bytes - 8 >= 0;      // TODO: Shouldn't need this with the new type checker
        // Grab the upper 64 bits and zero-out 1-7 of the bytes
        Mov64(tmp, num_bytes);
        Sub64(tmp, 8);      // Don't count the lower 64 bits
        Shl64(tmp, 3);      // tmp == 8 (bits/byte) * (num_bytes - 8)
        lemma_bytes_shift_power2(num_bytes - 8);
        assert tmp == 8 * (num_bytes - 8);
        Mov64(mask, 1);
        Shl64(mask, tmp);
        Sub64(mask, 1);
        Pextrq(tmp, io, 1);     
        ghost var old_upper128 := tmp;
        And64(tmp, mask);
        lemma_bytes_and_mod(old_upper128, num_bytes - 8); // ==>
        assert num_bytes - 8 >= 0 /\ tmp == old_upper128 % (pow2((num_bytes - 8) * 8));

        // Restore the upper 64 bits
        Pinsrq(io, tmp, 1);
        pad_to_128_bits_upper(old(io), num_bytes);
    }
}

#reset-options "--z3rlimit 10"
procedure {:quick} ghash_incremental_bytes_extra(
    ghost in_b:buffer128,
    ghost orig_in_ptr:nat64,
    ghost old_io:quad32,
    ghost orig_num_bytes:nat64
    )
    lets in_ptr @= r9; num_bytes @= rax; io @= xmm1; mask @= xmm8; h @= xmm11;

    reads
        num_bytes; h; mask; mem; in_ptr;

    modifies
        rcx; rdx; r12;
        io; efl; 

        // ReduceMul128_LE touches almost everything
        xmm2; xmm3; xmm4; xmm5; xmm6;

    requires
        // GHash reqs
        orig_num_bytes > 0 ==> validSrcAddrs128(mem, orig_in_ptr, in_b, bytes_to_quad_size(orig_num_bytes));
        orig_num_bytes > 0 ==> orig_in_ptr  + 16 * bytes_to_quad_size(orig_num_bytes) < pow2_64;
        buffer_length(in_b) == bytes_to_quad_size(orig_num_bytes);
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

        //len == 0 ==> rdx == old(rdx) /\ r9 == old(r9) /\ xmm1 == old(xmm1) /\ io == old(io);
        let input := slice_work_around(buffer128_as_seq(mem, in_b), orig_num_bytes / 16);
        io == ghash_incremental0(h, old_io, input);

        // Extra reqs
        let num_blocks := orig_num_bytes / 16;
        num_bytes == orig_num_bytes % 16;
        orig_num_bytes % 16 != 0;        // Note: This implies orig_num_bytes > 0
        0 < orig_num_bytes < 16 * bytes_to_quad_size(orig_num_bytes);
        16 * (bytes_to_quad_size(orig_num_bytes) - 1) < orig_num_bytes;
        in_ptr  == orig_in_ptr  + 16 * num_blocks;

    ensures
        let input_bytes := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem, in_b)), orig_num_bytes);
        let padded_bytes := pad_to_128_bits(input_bytes);
        let input_quads := le_bytes_to_seq_quad32(padded_bytes);
        orig_num_bytes > 0 ==> length(input_quads) > 0 /\ 
                          io == ghash_incremental(h, old_io, input_quads);
{
    ghost var num_blocks := orig_num_bytes / 16;

    Load128_buffer(xmm2, in_ptr, 0, in_b, num_blocks);  
    ghost var final_quad := xmm2;

    compute_pad_to_128_bits();
    ghost var final_quad_padded := xmm2;

    compute_ghash_incremental_register();
    
    lemma_ghash_incremental_bytes_extra_helper(h, old_io, old(io), io, buffer128_as_seq(mem, in_b), final_quad, final_quad_padded, orig_num_bytes);
}


#reset-options "--z3rlimit 10"
procedure {:quick} ghash_incremental_bytes(
    ghost in_b:buffer128
    )
    lets in_ptr @= rax; num_bytes @= r11; io @= xmm1; mask @= xmm8; h @= xmm11;

    reads
        num_bytes; h; mask; mem;

    modifies
        in_ptr; rcx; rdx; r9; r12;
        io; efl; 

        // ReduceMul128_LE touches almost everything
        xmm2; xmm3; xmm4; xmm5; xmm6;

    requires
        // GHash reqs
        num_bytes > 0 ==> validSrcAddrs128(mem, in_ptr, in_b, bytes_to_quad_size(num_bytes));
        num_bytes > 0 ==> in_ptr  + 16 * bytes_to_quad_size(num_bytes) < pow2_64;
        num_bytes > 0 ==> buffer_length(in_b) == bytes_to_quad_size(num_bytes);
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

    ensures
        num_bytes == 0 ==> rdx == old(rdx) /\ r9 == old(r9) /\ xmm1 == old(xmm1) /\ io == old(io);

        let input_bytes := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem, in_b)), num_bytes);
        let padded_bytes := pad_to_128_bits(input_bytes);
        let input_quads := le_bytes_to_seq_quad32(padded_bytes);
        num_bytes > 0 ==> length(input_quads) > 0 /\ 
            io == ghash_incremental(h, old(io), input_quads);
{
    ghost var num_blocks := old(num_bytes) / 16;

    lemma_poly_bits64();

    if (num_bytes > 0) {
        Mov64(rcx, num_bytes);
        Shr64(rcx, 4);
        assert rcx == num_blocks;
     
        compute_ghash_incremental_partial(in_b);

        Mov64(rax, num_bytes);
        And64(rax, 15);
        assert rax == num_bytes % 16;

        if (rax == 0) {
            ghash_incremental_bytes_no_extra(in_b, old(io), old(in_ptr), old(num_bytes), io, h);
        } else {
            ghash_incremental_bytes_extra(in_b, old(in_ptr), old(io), old(num_bytes)); 
        }
    }
}



procedure {:quick} ghash_core(
    ghost in_b:buffer128
    )
    lets in_ptr @= rax; len @= rcx; output @= xmm1; mask @= xmm8; h @= xmm11;

    reads
        in_ptr; len; h; mask;
        mem;

    modifies
        rdx; r9; r12;
        output; efl;

        // ReduceMul128_LE touches almost everything
        xmm2; xmm3; xmm4; xmm5; xmm6;
    requires
        // GHash reqs
        len > 0 ==> validSrcAddrs128(mem, in_ptr, in_b, len);
        len > 0 ==> in_ptr  + 16 * len < pow2_64;
        len > 0 ==> buffer_length(in_b) == len;
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

    ensures
        len == 0 ==> rdx == old(rdx) /\ r9 == old(r9) /\ xmm1 == old(xmm1) /\ output == old(output);
        len > 0 ==> length(buffer128_as_seq(mem, in_b)) > 0 /\ output == ghash_LE(h, buffer128_as_seq(mem, in_b));
{
    if (len != 0) {
        compute_Y0();
        compute_ghash_incremental(in_b);
        ghash_incremental_to_ghash(old(h), buffer128_as_seq(mem, in_b));
    }
}

