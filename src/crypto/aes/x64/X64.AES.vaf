include "../../../arch/x64/X64.Vale.InsBasic.vaf"
//include "../../../arch/x64/X64.Vale.InsMem.vaf"
include "../../../arch/x64/X64.Vale.InsVector.vaf"
//include "../../../arch/x64/X64.Vale.InsAes.vaf"
include "X64.AES128.vaf"
include "X64.AES256.vaf"

module X64.AES

#verbatim{:interface}{:implementation}
open Types_s
open FStar.Seq
open AES_s
open X64.Machine_s
open X64.Memory_i
open X64.Vale.State_i
open X64.Vale.Decls_i
open X64.Vale.InsBasic
//open X64.Vale.InsMem
open X64.Vale.InsVector
//open X64.Vale.InsAes
open X64.Vale.QuickCode_i
open X64.Vale.QuickCodes_i
open Types_i
//open AES_helpers_i
open AES256_helpers_i
//open Opaque_s
open X64.AES128
open X64.AES256
#endverbatim

#reset-options "--z3rlimit 20"

///////////////////////////
// KEY EXPANSION
///////////////////////////

procedure{:quick} KeyExpansionStdcall(
    inline win:bool,
    inline alg:algorithm,
    ghost input_key_b:buffer128,
    ghost output_key_expansion_b:buffer128)
    reads
        rcx; rsi; rdi;
    modifies
        rdx;
        mem; xmm1; xmm2; xmm3; xmm4; efl;
    lets
        key_ptr := if win then rcx else rdi;
        key_expansion_ptr := if win then rdx else rsi;
        // TODO: Vale's lack of subtyping won't allow this (or variants thereof), 
        // since each branch returns different type
        //key := (if (alg = AES_128) then quad32_to_seq(buffer128_read(input_key_b, 0, mem))
        //        else make_AES256_key(buffer128_read(input_key_b, 0, mem), buffer128_read(input_key_b, 1, mem)));
    requires/ensures
        alg = AES_128 || alg = AES_256;
        buffers_disjoint128(input_key_b, output_key_expansion_b); 
        validSrcAddrs128(mem, key_ptr, input_key_b, if alg = AES_128 then 1 else 2);
        validDstAddrs128(mem, key_expansion_ptr, output_key_expansion_b, nr(alg) + 1);
    ensures
        modifies_buffer128(output_key_expansion_b, old(mem), mem);
        // TODO: Revisit when Vale has native typing
        alg = AES_128 ==>
            (let key := old(quad32_to_seq(buffer128_read(input_key_b, 0, mem))) in
             forall j {buffer128_read(output_key_expansion_b, j, mem)} :: 0 <= j <= nr(alg) ==>
                 buffer128_read(output_key_expansion_b, j, mem) == index(key_to_round_keys_LE(alg, key), j));
        alg = AES_256 ==>
            (let key := old(make_AES256_key(buffer128_read(input_key_b, 0, mem), buffer128_read(input_key_b, 1, mem))) in
             forall j {buffer128_read(output_key_expansion_b, j, mem)} :: 0 <= j <= nr(alg) ==>
                 buffer128_read(output_key_expansion_b, j, mem) == index(key_to_round_keys_LE(alg, key), j));
{
    inline if (alg = AES_128) {
        KeyExpansion128Stdcall(win, input_key_b, output_key_expansion_b);
    } else {
        KeyExpansion256Stdcall(win, input_key_b, output_key_expansion_b);
    }
}

///////////////////////////
// ENCRYPTION
///////////////////////////

procedure{:quick} AESEncryptBlock(
    inline alg:algorithm,
    ghost input:quad32,
    ghost key:aes_key_LE(alg),
    ghost round_keys:seq(quad32),
    ghost keys_buffer:buffer128
    )
    reads
        r8; mem;
    modifies
        xmm0; xmm2; efl;
    requires
        alg = AES_128 || alg = AES_256;
        length(round_keys) == nr(alg) + 1;
        round_keys == key_to_round_keys_LE(alg, key);
        xmm0 == input;
        r8 == buffer_addr(keys_buffer, mem);
        validSrcAddrs128(mem, r8, keys_buffer, nr(alg) + 1);
        forall i:int :: 0 <= i < nr(alg) + 1 ==> buffer128_read(keys_buffer, i, mem) == index(round_keys, i);
    ensures
        xmm0 == aes_encrypt_LE(alg, key, input);
{
    inline if (alg = AES_128) {
        assert {:quick_type} alg == AES_128;
        AES128EncryptBlock(input, key, round_keys, keys_buffer);
    } else {
        assert {:quick_type} alg == AES_256;
        AES256EncryptBlock(input, key, round_keys, keys_buffer);
    }
}

procedure{:quick} AESEncryptBlockStdcall(
    inline win:bool,
    inline alg:algorithm,
    ghost input:quad32,
    ghost key:aes_key_LE(alg),
    ghost input_buffer:buffer128,
    ghost output_buffer:buffer128,
    ghost keys_buffer:buffer128
    )
    reads
        rcx; rdx; rsi; rdi;
    modifies
        r8;
        mem; xmm0; xmm2; efl;
    lets
        output_ptr := if win then rcx else rdi;
        input_ptr := if win then rdx else rsi;
        expanded_key_ptr := if win then r8 else rdx;
    requires
        alg = AES_128 || alg = AES_256;
        buffer128_read(input_buffer, 0, mem) == input;
        expanded_key_ptr == buffer_addr(keys_buffer, mem);
        validSrcAddrs128(mem, input_ptr, input_buffer, 1);
        validSrcAddrs128(mem, output_ptr, output_buffer, 1);
        validSrcAddrs128(mem, expanded_key_ptr, keys_buffer, nr(alg) + 1);
        forall i:int :: 0 <= i < nr(alg) + 1 ==>
            buffer128_read(keys_buffer, i, mem) == index(key_to_round_keys_LE(alg, key), i);
    ensures
        modifies_mem(loc_buffer(output_buffer), old(mem), mem);
        validSrcAddrs128(mem, output_ptr, output_buffer, 1);
        buffer128_read(output_buffer, 0, mem) == aes_encrypt_LE(alg, key, input);
{
    inline if (alg = AES_128) {
        assert {:quick_type} alg == AES_128;
        AES128EncryptBlockStdcall(win, input, key, input_buffer, output_buffer, keys_buffer);
    } else {
        assert {:quick_type} alg == AES_256;
        AES256EncryptBlockStdcall(win, input, key, input_buffer, output_buffer, keys_buffer);
    }
}






/*
///////////////////////////
// KEY INVERSION
///////////////////////////

procedure KeyInversionRound(
    inline round:int,
//    inline taint:taint,
    ghost k_b:buffer128)
//    ghost key:aes_key_LE(AES_128),
//    ghost w:seq(nat32),
//    ghost dw_in:seq(nat32)
//    ) returns (
//    ghost dw_out:seq(nat32)
//    )
    requires/ensures
        validSrcAddrs128(mem, rdx, k_b, 11);
    requires
        0 <= round <= 8;
//        SeqLength(w) == 44;
//        SeqLength(dw_in) == 4*(round+1);
//        KeyExpansionPredicate(key, AES_128, w);
//        EqInvkey_expansion_partial(key, AES_128, dw_in, round);
//        forall j :: 0 <= j <= round ==> mem[k_b].quads[rdx + 16*j].v == Quadword(dw_in[4*j], dw_in[4*j+1], dw_in[4*j+2], dw_in[4*j+3]);
//        forall j :: round < j <= 10 ==> mem[k_b].quads[rdx + 16*j].v == Quadword(w[4*j], w[4*j+1], w[4*j+2], w[4*j+3]);
    reads 
        rdx;
    modifies
        mem; xmm1; efl;
    ensures
        modifies_buffer_specific128(k_b, old(mem), mem, round+1, round+1);
//        forall a :: (a < rdx || a >= rdx + 176) && old(mem)[k_b].quads?[a] ==> mem[k_b].quads?[a] && mem[k_b].quads[a] == old(mem)[k_b].quads[a];
//        forall j :: round+1 < j <= 10 ==> mem[k_b].quads[rdx + 16*j].v == Quadword(w[4*j], w[4*j+1], w[4*j+2], w[4*j+3]);
//        SeqLength(dw_out) == 4*(round + 2);
//        forall j :: 0 <= j <= round+1 ==> mem[k_b].quads[rdx + 16*j].v == Quadword(dw_out[4*j], dw_out[4*j+1], dw_out[4*j+2], dw_out[4*j+3]);
//        EqInvkey_expansion_partial(key, AES_128, dw_out, round+1);
{
//    assert mem[k_b].quads[rdx + 16*(round+1)].v == Quadword(w[(round+1)*4], w[(round+1)*4 + 1], w[(round+1)*4 + 2], w[(round+1)*4 + 3]);

    Load128_buffer(xmm1, rdx, 16*(round+1), k_b, round+1);
//    assert xmm1 == Quadword(w[(round+1)*4], w[(round+1)*4 + 1], w[(round+1)*4 + 2], w[(round+1)*4 + 3]);
//    ghost var ws := SeqRange(w, (round+1)*4, (round+1)*4 + 4);
//    assert ws[0] == w[(round+1)*4] && ws[1] == w[(round+1)*4+1] && ws[2] == w[(round+1)*4+2] && ws[3] == w[(round+1)*4+3];
//    assert xmm1 == seq_to_Quadword(ws);
    AESNI_imc(xmm1, xmm1);
    Store128_buffer(rdx, xmm1, 16*(round+1), k_b, round+1);

//    dw_out := dw_in + Quadword_to_seq(xmm1);
//    lemma_KeyInversionRoundHelper(round+1, key, w, dw_in, dw_out);
//
//    forall j :| round+1 < j <= 10 :: mem[k_b].quads[rdx + 16*j].v == Quadword(w[4*j], w[4*j+1], w[4*j+2], w[4*j+3]) {
//    }
//
//    forall j :| 0 <= j <= round + 1 :: mem[k_b].quads[rdx + 16*j].v == Quadword(dw_out[4*j], dw_out[4*j+1], dw_out[4*j+2], dw_out[4*j+3]) {
//    }
}

procedure {:recursive} KeyInversionRoundUnrolledRecursive(
    inline rounds:int,
//    inline taint:taint,
    ghost k_b:buffer128)
//    ghost key:aes_key_LE(AES_128),
//    ghost w:seq(nat32),
//    ghost dw_in:seq(nat32)
//    ) returns (
//    ghost dw_out:seq(nat32)
//    )
    requires/ensures
        validSrcAddrs128(mem, rdx, k_b, 11);
    requires
        0 <= rounds <= 9;
        rdx % 16 == 0;
//        SeqLength(w) == 44;
//        SeqLength(dw_in) == 4;
//        KeyExpansionPredicate(key, AES_128, w);
//        EqInvkey_expansion_partial(key, AES_128, dw_in, 0);
//        mem[k_b].quads[rdx+16*0].v == Quadword(dw_in[4*0], dw_in[4*0+1], dw_in[4*0+2], dw_in[4*0+3]);
//        forall j :: 0 <= j <= 0 ==> mem[k_b].quads[rdx + 16*j].v == Quadword(dw_in[4*j], dw_in[4*j+1], dw_in[4*j+2], dw_in[4*j+3]);
//        forall j :: 0 < j <= 10 ==> mem[k_b].quads[rdx + 16*j].v == Quadword(w[4*j], w[4*j+1], w[4*j+2], w[4*j+3]);
    reads 
        rdx;
    modifies
        mem; xmm1; efl;
    ensures
        modifies_buffer128(k_b, old(mem), mem);
//        forall a :: (a < rdx || a >= rdx + 176) && old(mem)[k_b].quads?[a] ==> mem[k_b].quads?[a] && mem[k_b].quads[a] == old(mem)[k_b].quads[a];
//        SeqLength(dw_out) == 4*(rounds+1);
//        forall j :: 0 <= j <= rounds ==> mem[k_b].quads[rdx + 16*j].v == Quadword(dw_out[4*j], dw_out[4*j+1], dw_out[4*j+2], dw_out[4*j+3]);
//        forall j :: rounds < j <= 10 ==> mem[k_b].quads[rdx + 16*j].v == Quadword(w[4*j], w[4*j+1], w[4*j+2], w[4*j+3]);
//        EqInvkey_expansion_partial(key, AES_128, dw_out, rounds);
{
//    inline if (0 < rounds <= 9) {
//        ghost var dw_mid;
//        dw_mid := KeyInversionRoundUnrolledRecursive(rounds-1, taint, k_b, key, w, dw_in);
//        dw_out := KeyInversionRound(rounds-1, taint, k_b, key, w, dw_mid);
//    }
//    else {
//        dw_out := dw_in;
//    }
}

procedure KeyInversionImpl(
//    ghost key:aes_key_LE(AES_128),
//    ghost w:seq(nat32),
//    inline taint:taint,
    ghost k_b:buffer128)
//    ) returns (
//    ghost dw:seq(nat32)
//    )
    requires/ensures
        validSrcAddrs128(mem, rdx, k_b, 11);
    requires
        rdx % 16 == 0;
//        SeqLength(w) == 44;
//        forall j :: 0 <= j <= 10 ==> mem[k_b].quads[rdx + 16*j].v == Quadword(w[4*j], w[4*j+1], w[4*j+2], w[4*j+3]);
//        KeyExpansionPredicate(key, AES_128, w);
    ensures
        modifies_buffer128(k_b, old(mem), mem);
//        forall a :: (a < rdx || a >= rdx + 176) && old(mem)[k_b].quads?[a] ==> mem[k_b].quads?[a] && mem[k_b].quads[a] == old(mem)[k_b].quads[a];
//        SeqLength(dw) == 44;
//        EqInvKeyExpansionPredicate(key, AES_128, dw);
//        forall j :: 0 <= j <= 10 ==> mem[k_b].quads[rdx + 16*j].v == Quadword(dw[4*j], dw[4*j+1], dw[4*j+2], dw[4*j+3]);
    reads
        rdx;
    modifies
        mem; xmm1; efl;
{
//    lemma_KeyExpansionPredicateImpliesExpandKey(key, AES_128, w);
//    ghost var dw1 := seq(w[0], w[1], w[2], w[3]);
//
//    ghost var dw2;
//    dw2 := KeyInversionRoundUnrolledRecursive(9, taint, k_b, key, w, dw1);
    KeyInversionRoundUnrolledRecursive(9, k_b);
//
//    dw := dw2 + seq(w[40], w[41], w[42], w[43]);
//    assert SeqLength(dw) == 44;
//    forall j :| 0 <= j <= 10 :: mem[k_b].quads[rdx + 16*j].v == Quadword(dw[4*j], dw[4*j+1], dw[4*j+2], dw[4*j+3])
//    {
//    }
//    assert EqInvKeyExpansionPredicate(key, AES_128, dw);
}

procedure KeyExpansionAndInversionStdcall(
//    inline taint:taint,
    inline win:bool,
    ghost input_key_b:buffer128,
    ghost output_key_expansion_b:buffer128)
//    ) returns (
//    ghost dw:seq(nat32)
//    )
    reads
        rcx; rsi; rdi;
    modifies
        rdx;
        mem; xmm1; xmm2; xmm3; efl;
    requires
//        HasStackSlots(stack, 2);
        let key_ptr := if win then rcx else rdi;
        let key_expansion_ptr := if win then rdx else rsi;
        key_ptr % 16 == 0;
        key_expansion_ptr % 16 == 0;
        validSrcAddrs128(mem, key_ptr, input_key_b, 1);
        validDstAddrs128(mem, key_expansion_ptr, output_key_expansion_b, 11);
    ensures
        let key_ptr := if win then rcx else rdi;
        let key_expansion_ptr := if win then rdx else rsi;
//        let key := Quadword_to_seq(old(mem)[input_key_id].quads[key_ptr].v);
//        SeqLength(dw) == 44;
//        ValidSrcAddrs(mem, output_key_expansion_id, key_expansion_ptr, 128, taint, 176);
        validSrcAddrs128(mem, key_expansion_ptr, output_key_expansion_b, 11);
        modifies_buffer128(output_key_expansion_b, old(mem), mem);
//        (forall a :: (a < key_expansion_ptr || a >= key_expansion_ptr + 176) && old(mem)[output_key_expansion_id].quads?[a] ==> mem[output_key_expansion_id].quads?[a] && mem[output_key_expansion_id].quads[a] == old(mem)[output_key_expansion_id].quads[a]);
//        (forall j :: 0 <= j <= 10 ==> mem[output_key_expansion_id].quads[key_expansion_ptr + 16*j].v == Quadword(dw[4*j], dw[4*j+1], dw[4*j+2], dw[4*j+3]));
//        EqInvKeyExpansionPredicate(key, AES_128, dw);
{
    ghost var key_ptr := if win then rcx else rdi;
    ghost var key_expansion_ptr := if win then rdx else rsi;
//    ghost var key := Quadword_to_seq(mem[input_key_id].quads[key_ptr].v);

//    LoadStack(eax, 0);                                     // eax := key_ptr (from stack position 0)
    inline if (win)
    {
        Load128_buffer(xmm1, rcx, 0, input_key_b, 0);
    }
    else
    {
        Load128_buffer(xmm1, rdi, 0, input_key_b, 0);
        Mov64(rdx, rsi);
    }
//    ghost var w;
//    w := KeyExpansionImpl(key, taint, output_key_expansion_id); // expand key from xmm1 to region pointed to by rdx
//    dw := KeyInversionImpl(key, w, taint, output_key_expansion_id);
    ghost var key := quad32_to_seq(xmm1);
    KeyExpansionImpl(key, output_key_expansion_b); // expand key from xmm1 to region pointed to by rdx

//    forall j :| 0 <= j <= 10 :: mem[output_key_expansion_id].quads[key_expansion_ptr + 16*j].v == Quadword(dw[4*j], dw[4*j+1], dw[4*j+2], dw[4*j+3])
//    {
//        assert mem[output_key_expansion_id].quads[rdx + 16*j].v == Quadword(dw[4*j], dw[4*j+1], dw[4*j+2], dw[4*j+3]);
//        assert rdx == key_expansion_ptr;
//    }

    // Clear secrets out of registers
//    Xor32(eax, eax);
    Pxor(xmm1, xmm1);
    Pxor(xmm2, xmm2);
    Pxor(xmm3, xmm3);
}
*/
